<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="F. Plata, K. Iwanicki, M. Banaszek, W. Ciszewski" />
  <title>Distributed Systems Lab 14</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <style type="text/css">
body
{
font-family: Helvetica, sans;
background-color: #f0f0f0;
font-size: 12pt;
color: black;
text-decoration: none;
font-weight: normal;
}
section.content {
width: 19cm;
font-size: 12pt;
text-align: justify;
margin-left: auto;
margin-right: auto;
margin-top: 20pt;
background-color: white;
padding: 20pt;
}
h1
{
font-size: xx-large;
text-decoration: none;
font-weight: bold;
text-align: center;
}
h2
{
font-size: xx-large;
text-decoration: none;
font-weight: bold;
text-align: left;
border-bottom: 1px solid #808080;
}
h3
{
font-size: x-large;
text-decoration: none;
font-weight: bold;
text-align: left;
}
h1 + h3 {
text-align: center;
}
h4
{
font-size: large;
text-decoration: none;
font-weight: bold;
text-align: left;
}
h5
{
font-size: medium;
text-decoration: none;
font-weight: bold;
text-align: left;
}
h6
{
font-size: medium;
text-decoration: none;
font-weight: normal;
text-align: left;
}
table
{
border-width: 1px;
border-spacing: 0px;
border-style: solid;
border-color: #808080;
border-collapse: collapse;
font-family: Times, serif;
font-size: 12pt;
color: black;
text-decoration: none;
font-weight: normal;
background-color: white;
}
td
{
border-width: 1px;
border-style: solid;
border-color: #808080;
padding: 3pt;
background-color: white;
}
th
{
border-width: 1px;
border-style: solid;
border-color: #808080;
padding: 3pt;
font-weight: bold;
background-color: #f0f0f0;
}
a:link {
color: blue;
text-decoration: none;
font-weight: normal;
}
a:visited {
color: blue;
text-decoration: none;
font-weight: normal;
}
a:hover {
text-decoration: underline;
font-weight: normal;
}
pre.sourceCode {
font-size: 90%;
}
</style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="distributed-systems-lab-14" class="content">
<h1>Distributed Systems Lab 14</h1>
<h3 id="hashing-based-data-distribution">Hashing-based data distribution</h3>
<p>The previous labs focused largely on ensuring consistency of data in the face of concurrency and failures. This lab is in turn concerned with distributing data across machines comprising the system and efficiently localizing a machine responsible for a particular data item. It covers popular techniques known as <em>consistent hashing</em> and <em>distributed hash tables</em> and, as a running example, employs a solution named <em>Chord</em>. We will use files contained in <a href="./dslab14.tgz">this package</a>, which you should download and extract locally.</p>
<h2 id="learning-section">Learning Section</h2>
<p>Let us assume that we have a set of <code>N</code> machines, where <code>N</code> is potentially “large” and may change over time as machines come and go. Suppose also that we have a “huge” collection of data items, such that each data item has an associated <em>key</em> by which this item will be searched for. Our goal is to divide data items across the machines such that:</p>
<ol type="1">
<li><p>Each machine receives a similar number of data items and adding or removing a machine requires redistributing only a fraction of all data items.</p></li>
<li><p>Each data item can be efficiently looked up by any machine in the system, that is, given a transport-layer address of an arbitrary machine (i.e., its IP address and a port number) and the key of the data item, the system returns the transport-layer address of the machine that hosts the data item (if the item is present in the system).</p></li>
</ol>
<h3 id="consistent-hashing">Consistent hashing</h3>
<p>With respect to Requirement 1., a natural way of dividing the data items among the machines would involve classic hashing. In this approach, each machine would constitute a separate hash bucket, uniquely identified by some Machine ID, <code>MID</code> in <code>[0..N - 1]</code>. It would be responsible for storing those data items whose keys hash to its bucket, that is, every item whose Data ID, <code>DID = hash(key) mod N</code>, equals <code>MID</code>.</p>
<p>Unfortunately, such an approach does not guarantee that changing the machine population slightly would require only minimal changes to the data item distribution. On the contrary, changing the number of machines by as little as one requires redistributing virtually all data items as a result of their <code>DIDs</code> changing.</p>
<p>This problem is addressed by <strong>consistent hashing</strong>. In its popular variant, all IDs belong to a large numerical space <code>[0..2^B - 1]</code>, which conceptually loops around, thereby forming a ring modulo <code>2^B</code>, that is, the next ID after <code>2^B - 1</code> is <code>0</code>. Constant <code>B</code> denotes the length of an identifier in bits and is normally at least 128. Each machine is assigned its <code>MID</code> from this space uniformly at random, so that the machines are evenly dispersed on the ring. Likewise, the <code>DID</code> for a data item is obtained by hashing the key of the item to the ID space with a cryptographic hash function, so that <code>DIDs</code> are also expected to be uniformly distributed across the ring. Importantly, however, in contrast to classic hashing, each machine is responsible not only for those data items whose <code>DIDs</code> are equal to its <code>MID</code>, but for all those data items whose <code>DIDs</code> fall into a “specific range” on the ring around the <code>MID</code>. While the definition of “specific range” may vary between particular solutions, it ensures that adding or removing a machine affects only the two closest machines: the immediate successor and predecessor of the machine on the ring.</p>
<p>In Chord, which is our running example of an actual solution for this lab, the notion of “specific range” is defined as follows: a node with a given <code>MID</code> is responsible for all <code>DIDs</code> that are equal to the <code>MID</code> or fall between this <code>MID</code> and the one of the preceding active machine on the ring. For example, suppose that <code>B = 5</code> (i.e., the maximal ID is 31) and that machines with the following <code>MIDs</code> are active in the system: <code>0</code>, <code>3</code>, <code>8</code>, <code>10</code>, <code>13</code>, <code>17</code>, <code>19</code>, <code>20</code>, and <code>27</code>. In such a setting, the machine with <code>MID = 19</code> (machine <code>19</code>) is responsible for <code>DIDs</code> <code>{18, 19}</code> as it succeeds on the ring the machine with <code>MID = 17</code> (machine <code>17</code>). Accordingly, machine <code>20</code> is responsible only for <code>DID</code> <code>{20}</code>, machine <code>27</code> for <code>{21, ..., 27}</code>, and machine <code>0</code> for <code>{28, ..., 31, 0}</code>. The complete mapping of <code>DIDs</code> to <code>MIDs</code> in the example is illustrated in <a href="./chord-hashing.png">this diagram</a>.</p>
<p>As a side note, in practice, a single <code>DID</code> is mapped to multiple <code>MIDs</code> so that simultaneous failures of several machines can be tolerated. However, such redundancy rules are relatively straightforward and are thus immaterial for today’s lab.</p>
<h3 id="distributed-hash-table">Distributed hash table</h3>
<p>Requirement 2., the ability to look up data items, can be ensured in a centralized manner: by having a dedicated (possibly replicated) server that monitors all machines, maintains the <code>DID</code>-to-<code>MID</code> mapping, updates it whenever the machine population changes, and propagates it back to each machine. In fact, this solution is quite common in practice.</p>
<p>In systems involving very large numbers of machines, however, a centralized solution may not scale. Instead, such systems may require maintaining the <code>DID</code>-to-<code>MID</code> mapping in a decentralized fashion. Such functionality is referred to as a <strong>distributed hash table</strong> (<strong>DHT</strong>).</p>
<p>Many DHTs have been proposed. Their common property is that they are implemented as <strong>overlay networks</strong>. Such a network is composed of <em>nodes</em> and <em>links</em>. In our case, a <em>node</em> corresponds to a machine (or actually an instance of the process implementing the DHT functionality on this machine). In any case, each node knows only about a small subset of all nodes. The knowledge that a node has about another node includes information on how the other node can be reached (e.g., its transport-layer address). This information can thus be thought of as a (logical) <em>link</em> in the overlay network: by having such a link, the node can send messages to the other node; in contrast, it is unable to initiate communication with a node it has no link to (no information about). A DHT uses its overlay links to route a message with a given key between its nodes, so that the message ultimately reaches a node responsible for this key (actually for the corresponding <code>DID</code>). This routing is done at the application layer of the OS network protocol stack, and hence is called <strong>overlay routing</strong> to emphasize that it is done on top of the regular Internet routing. The particular DHTs differ in their rules on selecting overlay links and routing messages. In particular, during the lecture, rules for the <em>Pastry</em> DHT were given; here, in turn, we explain the ones for <em>Chord</em>.</p>
<h4 id="overlay-construction-rules-in-chord">Overlay construction rules in Chord</h4>
<p>In Chord, a link to a node comprises, among others, the <code>MID</code> and the transport-layer address of this node. Each node maintains locally three data structures with links to <em>other</em> nodes:</p>
<ul>
<li><p><em>successor table</em> – Contains up to <code>R</code> links to nodes succeeding the present node on the ring (where <code>R &gt;= 1</code> is a configuration parameter). The <code>0</code>-th entry of the table is a link to the next active node on the ring in the clockwise order (if it exists), the <code>1</code>-st entry – to the second next node in this order, and so on.</p></li>
<li><p><em>predecessor table</em> – By symmetry, contains up to <code>R</code> links to nodes preceding the present node on the ring. The <code>0</code>-th entry of the table is a link to the next active node in the counter-clockwise order on the ring (if it exists), the <code>1</code>-st entry – to the second next node in this order, and so on.</p></li>
<li><p><em>finger table</em> – Contains up to <code>B</code> links that are shortcuts in the ring. More specifically, the <code>i</code>-th element of the table is a link to the first active node on the ring in the clockwise order whose <code>MID</code> is at least <code>2^i</code> apart from the <code>MID</code> of the present node but less than <code>2^(i+1)</code> apart (if such a node exists).</p></li>
</ul>
<p>The three tables constitute the <em>routing state</em> of a node. It can be observed that this state scales logarithmically with <code>N</code> (assuming a uniform <code>MID</code> distribution on the ring).</p>
<p>For illustration, consider the previous example of a ring. If <code>R</code> was <code>3</code>, then the successor and predecessor tables of machine <code>8</code> would be <code>[10, 13, 17]</code> and <code>[3, 0, 27]</code>, respectively. In contrast, if <code>R</code> were greater than <code>8</code>, then only the initial <code>8</code> entries would be present in the two tables of any machine (the other being <code>null</code>). When it comes in turn to the finger tables, for machine <code>8</code> it would be as follows: <code>[null, 10, 13, 17, 27]</code> because there is no machine with its <code>MID</code> in range <code>[8+2^0..8+2^1)</code>, the first machine in the clockwise order with its <code>MID</code> in range <code>[8+2^1..8+2^2)</code> is machine <code>10</code>, the first machine in range <code>[8+2^2..8+2^3)</code> is machine <code>13</code>, and so on. Accordingly, the finger table of machine <code>19</code> would be <code>[20, null, null, 27, 3]</code>. The routing states of all nodes in the example, assuming <code>R = 1</code>, are illustrated in <a href="./chord-state.png">this diagram</a>.</p>
<h4 id="routing-in-chord">Routing in Chord</h4>
<p>The routing states allow individual nodes to forward messages for specific <code>DIDs</code> over the overlay links so that the messages ultimately reach the nodes responsible for the <code>DIDs</code>. To this end, each node receiving a message inspects the destination <code>DID</code> in the message and its local routing state and decides what to do with the message as follows:</p>
<ol type="1">
<li><p>If, based on its predecessor and successor tables and its own <code>MID</code>, the node is able to determine the target <code>MID</code> responsible for the <code>DID</code>, then it forwards the message using the transport-layer address associated with the link to that <code>MID</code> (if the target <code>MID</code> belongs to another node) or accepts the message as the destination node (if the target <code>MID</code> equals its own <code>MID</code>).</p></li>
<li><p>Otherwise, it uses its finger table greedily: it selects for forwarding the entry with the largest index in the table such that sending the message to the <code>MID</code> corresponding to the entry does not result in “jumping” in the clockwise order over the <code>DID</code> on the ring. (In other words, it forwards the message as far as it is possible but not to overjump the <code>DID</code>.)</p></li>
</ol>
<p>As an example, consider again the aforementioned system (with <code>B = 5</code> and <code>R = 1</code>), depicted in <a href="./chord-routing.png">this diagram</a>. Routing a message for <code>DID = 3</code> by machine <code>8</code> can be done in one hop because, from its predecessor table, the machine can determine that machine <code>3</code> is responsible for the <code>DID</code>. Likewise, routing a message for <code>DID = 12</code> by node <code>10</code> can be done in one hop because, from its successor table, machine <code>10</code> can determine that machine <code>13</code> is responsible for the <code>DID</code>. In contrast, routing a message for <code>DID = 25</code> by machine <code>0</code> requires four hops:</p>
<ul>
<li><p>Machine <code>0</code> uses the 4-th entry in its finger table as it is the largest-index entry and does not cause jumping over <code>25</code> in the clockwise order on the ring. In effect, the message is forwarded to machine <code>17</code>.</p></li>
<li><p>Machine <code>17</code> uses the 1-st entry in its finger table because the 2-nd entry is <code>null</code> while forwarding to the 3-rd entry would result in jumping over <code>25</code> in the clockwise order on the ring. As a result, the message is received by machine <code>19</code>.</p></li>
<li><p>Machine <code>19</code> uses the 0-th entry in its finger table for the same reasons, thereby sending the message to machine <code>20</code>.</p></li>
<li><p>Machine <code>20</code> determines, based on its successor table, that it is machine <code>27</code> that is responsible for <code>DID = 25</code>, so it forwards the message to that machine, which constitutes the last transmission as machine <code>27</code> accepts the message based on its predecessor table and its own <code>MID</code>.</p></li>
</ul>
<p>In general, like routing state, the maximal number of hops scales logarithmically with <code>N</code> (assuming a uniform distribution of <code>MIDs</code> over the ring). Therefore, at least from the algorithmic perspective, Chord is scalable overall: if the system doubles, its asymptotic performance drops only by a constant factor.</p>
<h4 id="final-remarks">Final remarks</h4>
<p>The presented rules concern only a system in a stable state. In the real world, however, the state of particular nodes may be inconsistent, notably during changes in the node population. Moreover, there are many other issues a practical implementation has to consider. For more information, refer to the additional homework.</p>
<h2 id="small-assignment">Small Assignment</h2>
<p>Your task is to implement in Rust the link selection and routing rules described for Chord in this scenario. You shall complete the implementation of two functions provided in the template, following doc comments defined there.</p>
<p>This Small Assignment is worth <strong>2 points</strong>: 1 point if the link selection rules are implemented correctly and 1 point if routing is implemented correctly. To run the code you should use the executor system you developed as the first Large Assignment.</p>
<h2 id="additional-homework">Additional Homework</h2>
<p>In you are interested in Chord, you can start with the <a href="https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf">original Chord paper</a>. The paper presents mostly theoretical concepts, while implementing them in practice requires further solutions, some of which are described in the following <a href="https://pdos.csail.mit.edu/papers/fdabek-phd-thesis.pdf">PhD dissertation</a>.</p>
<p>As an alternative to Chord, you can look at Pastry, which was developed roughly at the same time and was more mature upon its introduction (Pastry was discussed during the lecture). In particular, the <a href="https://www.microsoft.com/en-us/research/project/pastry/">website of the project</a> and of <a href="https://www.microsoft.com/en-us/research/people/antr/">its key contributor</a> may be a good start.</p>
<p>All in all, DHTs are an important concept in distributed systems, and hence there are myriads of publications concerned with them.</p>
<hr />
<p>Authors: K. Iwanicki, W. Ciszewski, M. Banaszek.</p>
</section>
</body>
</html>
