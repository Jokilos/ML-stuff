<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="F. Plata, K. Iwanicki, M. Banaszek, W. Ciszewski" />
  <title>Distributed Systems Lab 06</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <style type="text/css">
body
{
font-family: Helvetica, sans;
background-color: #f0f0f0;
font-size: 12pt;
color: black;
text-decoration: none;
font-weight: normal;
}
section.content {
width: 19cm;
font-size: 12pt;
text-align: justify;
margin-left: auto;
margin-right: auto;
margin-top: 20pt;
background-color: white;
padding: 20pt;
}
h1
{
font-size: xx-large;
text-decoration: none;
font-weight: bold;
text-align: center;
}
h2
{
font-size: xx-large;
text-decoration: none;
font-weight: bold;
text-align: left;
border-bottom: 1px solid #808080;
}
h3
{
font-size: x-large;
text-decoration: none;
font-weight: bold;
text-align: left;
}
h1 + h3 {
text-align: center;
}
h4
{
font-size: large;
text-decoration: none;
font-weight: bold;
text-align: left;
}
h5
{
font-size: medium;
text-decoration: none;
font-weight: bold;
text-align: left;
}
h6
{
font-size: medium;
text-decoration: none;
font-weight: normal;
text-align: left;
}
table
{
border-width: 1px;
border-spacing: 0px;
border-style: solid;
border-color: #808080;
border-collapse: collapse;
font-family: Times, serif;
font-size: 12pt;
color: black;
text-decoration: none;
font-weight: normal;
background-color: white;
}
td
{
border-width: 1px;
border-style: solid;
border-color: #808080;
padding: 3pt;
background-color: white;
}
th
{
border-width: 1px;
border-style: solid;
border-color: #808080;
padding: 3pt;
font-weight: bold;
background-color: #f0f0f0;
}
a:link {
color: blue;
text-decoration: none;
font-weight: normal;
}
a:visited {
color: blue;
text-decoration: none;
font-weight: normal;
}
a:hover {
text-decoration: underline;
font-weight: normal;
}
pre.sourceCode {
font-size: 90%;
}
</style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="distributed-systems-lab-06" class="content">
<h1>Distributed Systems Lab 06</h1>
<h3 id="stable-storage">Stable storage</h3>
<p>During the previous lab, we discussed secure network communication between processes of a distributed system and presented how to implement it in Rust. Another crucial abstraction, especially in distributed algorithms for the crash-recovery failure model, is <em>stable storage</em>. This lab thus presents stable storage and discusses how to implement it on top a filesystem. To this end, we will use files contained in <a href="./dslab06.tgz">this package</a>, which you should download and extract locally.</p>
<h2 id="learning-section">Learning section</h2>
<h3 id="stable-storage-1">Stable storage</h3>
<p>In the <strong>crash-recovery failure model</strong>, when a process crashes, the content of its transient memory is lost. However, usually a recovering process should resume its operation rather than restart it from the beginning. To this end, it needs to be able to recover its state (e.g., a current epoch number or a set of pending messages) from before the crash. It is possible if the state is stored in <strong>stable storage</strong>, sometimes called also <strong>persistent storage</strong>.</p>
<p>During this course, we will use stable storage that implements a <strong>key-value</strong> interface. Such storage provides two basic <strong>atomic operations</strong>: <code>store</code> and <code>retrieve</code>. A value can be stored (written) under some key using <code>store</code> and then retrieved (read) via the same key using <code>retrieve</code>. Once the <code>store</code> operation completes successfully (which in practice means that a call to the corresponding method returns), the written value is reliably saved in the storage and can be read (using <code>retrieve</code>) even after process crashes. A subsequent <code>store</code> with the same key reliably overwrites the saved value.</p>
<p>The stable storage you will be implementing in today’s Small Assignment additionally includes a <code>remove</code> operation, which gives guarantees analogous to the ones of <code>store</code>. Note that while in practice a <code>remove</code> operation might be convenient and used to reduce the size of stored data, it is not necessary for theoretical algorithms, as it can be emulated by overwriting a key’s data with a special “nothing” value.</p>
<h4 id="stable-storage-on-top-a-filesystem">Stable storage on top a filesystem</h4>
<p>An implementation of stable storage can be realized on top of a filesystem provided by an operating system. Strictly speaking, such stable storage requires an assumption that the implementation of the filesystem fully adheres to system call specification, is bug-free and that the underlying drive will not fail. However, the required level of fault tolerance can be, in practice, achieved relatively easy by using some mature filesystem and by increasing the robustness of hardware using information redundancy (e.g., RAID 1). As always, we cannot be <em>absolutely certain</em> a single machine would recover, but it should be <em>virtually impossible</em> to observe the storage in an inconsistent state.</p>
<p>To implement stable storage on top of a filesystem, an ability to reliably and atomically write data is required. On a POSIX system, we can rely on the <a href="https://linux.die.net/man/2/fsync"><code>fsync</code> and <code>fsyncdata</code></a> system calls as primitive building blocks. These calls block until all modified data of a specified file descriptor is flushed onto the disk. The minor difference between those calls is that <code>fsync</code> always flushes metadata associated with the descriptor, while <code>fsyncdata</code> flushes only these strictly necessary. Neither of them updates the list of files in the containing directory, and any failure should be considered fatal for the system.</p>
<p>With such an assumption, we can use the following scheme to store data the data in <code>dstdir/dstfile</code>:</p>
<ol type="1">
<li>Write the data with a checksum (e.g., CRC32) to a temporary file <code>dstdir/tmpfile</code>.</li>
<li>Call the POSIX <code>fsyncdata</code> function on <code>dstdir/tmpfile</code> to ensure the data is actually transferred to a disk device (in Rust, one can use the <code>tokio::fs::File::sync_data()</code> method).</li>
<li>Call <code>fsyncdata</code> on <code>dstdir</code> to transfer the data of the modified directory to the disk device. (Again, in Rust, one can use the <code>tokio::fs::File::sync_data()</code> method. Even though the struct is called <code>File</code>, here it can be used for directories as well, for example: <code>tokio::fs::File::open(&quot;dir&quot;).await.unwrap().sync_data().await.unwrap()</code>).</li>
<li>Write the data (without the checksum) to <code>dstdir/dstfile</code>.</li>
<li>Call <code>fsyncdata</code> on <code>dstdir/dstfile</code>.</li>
<li>Call <code>fsyncdata</code> on <code>dstdir</code> (only necessary if <code>dstfile</code> did not exist before the previous step).</li>
<li>Remove <code>dstdir/tmpfile</code>.</li>
<li>Call <code>fsyncdata</code> on <code>dstdir</code>.</li>
</ol>
<p>On recovery, one needs to check the <code>tmpfile</code>. If it does not exist, this means that <code>dstfile</code> contains valid data (the most recent write was not interrupted). On the other hand, if it exists: - If the checksum is incorrect, this means that a crash during a write caused a corrupted <code>tmpfile</code>. This crash must have happened before <code>dstfile</code> was modified, so <code>tmpfile</code> can be simply removed, thus cancelling the write. - If the checksum is correct, <code>tmpfile</code> was fully written and a crash happened later. In this case the write can be resumed using the data from <code>tmpfile</code> (regardless of whether <code>dstfile</code> is corrupted).</p>
<p>Note that this scheme is atomic with respect to crashes, but it is not atomic with respect to concurrent accesses. Conversely, the <code>rename</code> function guarantees atomicity with respect to concurrent accesses, but not with respect to crashes. Furthermore, notice we cover data integrity only across crashes, and rely on filesystem integrity checks otherwise.</p>
<h2 id="small-assignment">Small Assignment</h2>
<p>Your task is to write a Rust implementation of stable storage on top of a filesystem directory. The stable storage shall implement the <code>StableStorage</code> trait, which defines key-value storage having the following properties:</p>
<ul>
<li>The <code>put()</code> method implements the <code>store</code> operation described above.</li>
<li>The <code>get()</code> methods implements the <code>retrieve</code> operation described above.</li>
<li>The <code>remove()</code> method implements the <code>remove</code> operation described above.</li>
<li>The storage supports keys that are at most 255 bytes long, and values that are at most 65535 bytes long.</li>
<li>An attempt to store an invalid key or an invalid value returns <code>Err</code> with an error message of your choosing. The attempt does not result in a malfunction of the storage.</li>
<li>Likewise, removing a nonexistent key does not corrupt the storage and just returns <code>false</code>.</li>
<li>If a key has been successfully inserted, updated, or removed before a crash (i.e., a <code>put()</code> ended with <code>Ok</code> or a <code>remove()</code> ended with any result), after the restart the storage returns the most recently stored value for that key (or <code>None</code> if the most recent operation for that key was <code>remove</code>). This includes crashes of the operating system too.</li>
<li>Values are stored atomically: either a whole value is stored under a key or removed, or the value is not inserted/updated/removed at all.</li>
<li>The <code>get()</code> method returns <code>None</code> for a key that has been removed or has never been inserted.</li>
<li>The <code>remove()</code> method returns <code>true</code> if the call actually removed a key and its data; otherwise, the method returns <code>false</code>.</li>
</ul>
<p>Your implementation shall also provide the <code>build_stable_storage()</code> function, which returns a trait object implementing the stable storage.</p>
<p>The execution time and number of accessed files for any stable storage operation (including recovery) should be independent of the number of keys kept in the storage (assuming the time it takes to read, write, or remove a file in a directory is independent of the number of files in that directory).</p>
<p>If a filesystem operation fails (e.g., due to insufficient disk space), your solution shall panic. It must not make the stable storage enter an invalid state.</p>
<p>You can assume that the directory provided to the <code>build_stable_storage()</code> function already exists and that it will be used exclusively by your stable storage instance.</p>
<p>You can store all keys in memory, but not values.</p>
<p>You are allowed to create subdirectories within the provided directory. You are not allowed to touch any other directories.</p>
<p>You can use the <code>sha2</code> and <code>base64</code> crates, as imported in <code>Cargo.toml</code>. If you use <code>sha2</code>, you can assume that there will be no SHA-256 collisions.</p>
<p>The second Large Assignment may require implementing such stable storage, and you can then reuse your solution for this assignment.</p>
<h2 id="additional-homework">Additional Homework</h2>
<p>Observe that databases are an excellent example of systems requiring stable storage: after a transaction is committed, the modified data are expected to be stored reliably despite any software and hardware failures. One of the techniques used by databases to provide stable storage for complex data is a <strong>Write Ahead Log (WAL)</strong>. You can read more about it in a <a href="https://medium.com/@daniel.chia/writing-a-database-part-2-write-ahead-log-2463f5cec67a">blog post</a> by Daniel Chia. The concept of using a log to guarantee reliable data operations is used also in <strong>journaling filesystems</strong>. You can read about them in this <a href="https://en.wikipedia.org/wiki/Journaling_file_system">Wikipedia article</a>. In general, <a href="https://en.wikipedia.org/wiki/Append-only">append-only data structures</a> are easier to implement safely. However, in order to prevent the data storage from growing uncontrollably, an atomic compaction operation is required.</p>
<p>If you wonder how many bugs can be present in a popular filesystem implementation, we recommend reading the <a href="http://pages.cs.wisc.edu/~ll/papers/fsstudy-tos.pdf">A Study of Linux File System Evolution</a> paper by L. Lu et al., in which the authors examine over 5000 patches from six major filesystems available in Linux.</p>
<p>In the lab, we assumed correctness of the system primitives. However, the practical world is full of corner cases and underspecified behavior. For instance, a hard drive is some configurations may lie about storing the transfer in non-volatile memory, while keeping them in an internal battery-powered buffer. On the other hand, most of the popular filesystems trust the disk for verifying data integrity against corruption, but these checksums are relatively weak. You may find these links useful to deepen your knowledge:</p>
<ul>
<li><a href="https://wiki.postgresql.org/wiki/Fsync_Errors">PostgreSQL fsyncgate 2018</a>: can an application not notice a write failure?</li>
<li><a href="https://www.usenix.org/system/files/atc20-rebello.pdf">Can Applications Recover from fsync Failures?</a>: a research paper analyzing various filesystems and applications.</li>
<li>Linux Kernel documentation on <a href="https://www.kernel.org/doc/Documentation/block/data-integrity.txt">block device data integrity</a> written in 2007.</li>
<li><a href="https://danluu.com/deconstruct-files/">Files are fraught with peril</a>: a transcript of a talk covering overall guaranteed persistence.</li>
</ul>
<hr />
<p>Authors: F. Plata, K. Iwanicki, M. Banaszek, W. Ciszewski, M. Matraszek.</p>
</section>
</body>
</html>
