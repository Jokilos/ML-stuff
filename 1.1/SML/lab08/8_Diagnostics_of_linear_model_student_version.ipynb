{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "685bcedd"
   },
   "source": [
    "# Statistical Machine Learning -- Notebook 8, version for students\n",
    "**Author: Dorota Celińska-Kopczyńska, Michał Ciach**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAMEuAbbH-Or"
   },
   "source": [
    "## Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXweAU-UR7F7"
   },
   "source": [
    "In today's class, we will work with the model's diagnostics. We will be checking whether the linear regression assumptions are satisfied in our sample datasets. We will also discuss the individual and joint significance of the parameters in our model and look for any influential observations.\n",
    "\n",
    "The importance of the diagnostics depends on the focus of our model. Generally, while working with linear regression, we may be primarily interested in either a) interpretability or b) prediction. If our main focus is interpretability, we have to pay special attention to the properties of the OLS estimator such as unbiasedness or efficiency. In such a situation, diagnostics is a must, i.e., if some assumptions are not satisfied in our model, the interpretability will be questionable. On the other hand, even a model that does not satisfy all assumptions may be beneficial for prediction.  \n",
    "\n",
    "The assumptions we will check can be summarized as the LINE rule:    \n",
    "\n",
    "- **L**inear trend,\n",
    "- **I**ndependent residuals (lack of autocorrelation),\n",
    "- **N**ormally distributed residuals,\n",
    "- **E**qual variance of residuals for all values of independent variables (homoscedasticity).\n",
    "\n",
    "We will check them visually by creating and analyzing the following diagnostic plots:   \n",
    "\n",
    "- The residual value vs the fitted value,\n",
    "- The root square of the absolute value of standardized residuals vs the fitted value,\n",
    "- The line plot of the residuals,\n",
    "- Graphical analysis of the distribution of the residuals (histogram, boxplots, qq-plot).\n",
    "\n",
    "The first plot is used to check if the relationship between the response (the dependent variable) and the predictors (the independent variables) is linear and to very roughly check if the residuals are uncorrelated. We expect values distributed symmetrically across the line $y=0$. However, this plot may be misleading if a non-spherical random disturbance occurs. That is why we encourage performing the Ramsey RESET test.  \n",
    "\n",
    "The second plot checks the variance's homoscedasticity (equality for all values of the independent variables). We expect values to be distributed symmetrically across a straight horizontal line.\n",
    "\n",
    "The third plot allows you to determine if your model has problems with autocorrelation (or heteroscedasticity). We expect to see a plot resembling a sound wave of constant amplitude (homoscedasticity) and no trend (lack of autocorrelation).\n",
    "\n",
    "The histogram is used to visualize the distribution of residuals. You can also use a qq-plot if you know how to create and interpret it.\n",
    "\n",
    "While the above are the assumptions of the classical linear regression model, some problems with data might still occur and may significantly reduce the quality of the results obtained with OLS. That is why, in the later part of this notebook, we will also address the issues of multicollinearity and influential observations and how to find them. For the detection of the multicollinearity, we will use VIF. Finally, we will inspect either the influence plot or the leverage-resid2 plot,  implemented in [`statsmodels`](https://www.statsmodels.org/dev/examples/notebooks/generated/regression_plots.html). Both plots are used to detect outliers that highly influence the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkB6U888K8KX",
    "outputId": "f608c0ce-4d28-4ce0-f06b-e40a2ffe15eb"
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!pip install --upgrade gdown\n",
    "!gdown https://drive.google.com/uc?id=1PjeSdrN9E0fzs3J0zVT7P7a1PT1vMDZ6\n",
    "!gdown https://drive.google.com/uc?id=1vne1N6D0yov8lrp9kCo1qZoT61CXYL6P\n",
    "!gdown https://drive.google.com/uc?id=1k0iAQYLdppDQUBmKPZ0ckSNLNv9izwZU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbgBtcAsK6T2"
   },
   "source": [
    "## Data & library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8H2cI48aR97y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dofE7aHSGpk"
   },
   "outputs": [],
   "source": [
    "income = pd.read_csv('6. BDL municipality incomes 2015-2020.csv', sep=';', dtype={'Code': 'str'})\n",
    "population = pd.read_csv('6. BDL municipality population 2015-2020.csv', sep='\\t', dtype={'Code': 'str'})\n",
    "area = pd.read_csv('6. BDL municipality area km2 2015-2020.csv', sep='\\t', dtype={'Code': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4ryDcevSVBx"
   },
   "outputs": [],
   "source": [
    "voivodeship_names = {\n",
    "    '02': 'Dolnośląskie',\n",
    "    '04': 'Kujawsko-pomorskie',\n",
    "    '06': 'Lubelskie',\n",
    "    '08': 'Lubuskie',\n",
    "    '10': 'Łódzkie',\n",
    "    '12': 'Małopolskie',\n",
    "    '14': 'Mazowieckie',\n",
    "    '16': 'Opolskie',\n",
    "    '18': 'Podkarpackie',\n",
    "    '20': 'Podlaskie',\n",
    "    '22': 'Pomorskie',\n",
    "    '24': 'Śląskie',\n",
    "    '26': 'Świętokrzyskie',\n",
    "    '28': 'Warmińsko-mazurskie',\n",
    "    '30': 'Wielkopolskie',\n",
    "    '32': 'Zachodniopomorskie'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5-3Z6mDSVZt"
   },
   "outputs": [],
   "source": [
    "code_list = [s[:2] for s in income[\"Code\"]]\n",
    "name_list = [voivodeship_names[code] for code in code_list]\n",
    "income['Voivodeship'] = name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxWbZU5XLYCk"
   },
   "source": [
    "## Diagnostics when assumptions are satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI3nNvJRLdEv"
   },
   "source": [
    "**Exercise 1.** In this exercise, we will inspect the diagnostics of the model when the assumptions of the linear regression model are satisfied. We will focus on artificial data with a given multidimensional distribution and correlation matrix. Our variables are not highly correlated with each other. We consider a model in which y will be the dependent variable (response), and A, B, C, and the constant will be the independent ones (predictors).\n",
    "\n",
    "The code below shows you how to generate data from a multivariate normal distribution. In [scipy.stats.multivariate_normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html), you will find the function `rvs()`, which samples from a multivariate normal distribution. We need to specify the means of the variables and the covariance matrix (note that we provide variances on the diagonal of the matrix). For the replicability of the results, we also specify the seed -- this way every time you run this code, you will get exactly same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "cfA_79TyyOZS",
    "outputId": "2fdbd581-1983-4c35-a98d-6e601acba7f4"
   },
   "outputs": [],
   "source": [
    "# generate the artificial data -- X matrix\n",
    "# construct the covariance matrix\n",
    "cov = np.array([[1.0, 0.0, 0.0],[0.0,25.0,4.0],[0.0,4.0,25.0]])\n",
    "cov\n",
    "\n",
    "# set the seed for reproducibility\n",
    "np.random.seed(17042023)\n",
    "\n",
    "df = mvn.rvs(mean = [0.0,20.0,20.0], cov=cov, size = 1000)\n",
    "\n",
    "X = pd.DataFrame(df, columns=['A', 'B', 'C'])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6uHztM7JMYx"
   },
   "source": [
    "Our data are generated according to the linear model: $y = 0.4A + 0.5B + 0.5C + \\varepsilon$. Note that this will be our ground truth -- the scenario we know is true, and we will compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "rtO2J6Im45Cv",
    "outputId": "d1c8049f-63c1-4e40-b977-c94d2e5d6ab4"
   },
   "outputs": [],
   "source": [
    "# generate artificial data -- Y\n",
    "Y = 0.4*X['A'] + 0.5*X['B'] + 0.5*X['C'] + np.random.normal(0,1,1000)\n",
    "Y = pd.DataFrame(Y, columns=['Y'])\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2ML3DQNJiEi"
   },
   "source": [
    "**Exercise 1a**. Inspect the descriptive statistics of the data set.\n",
    "Visualize the relationships between dependent and independent variables.\n",
    "\n",
    "*Hint:* for a convenient preparation of scatterplots for all of the pairs of the variables in your dataset, you may use [`scatter_matrix`](https://plotly.github.io/plotly.py-docs/generated/plotly.express.scatter_matrix.html) from `plotly.express`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "76mLuFgv6ymM",
    "outputId": "b1eeda98-fd6b-44e1-f482-b5b195e0fc6c"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hMFXpjPSNLD"
   },
   "source": [
    "**Exercise 1b** Estimate the model and inspect its summary. Are the variables jointly significant according to the F-test? Are all individual variables significant according to the t-test? Compare the estimates' values with the parameters' true values (from the ground truth).\n",
    "\n",
    "Hint: Not that the intercept is not included by default and we need to modify our dataset with [sm.add_constant()](https://www.statsmodels.org/dev/generated/statsmodels.tools.tools.add_constant.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdLLEbfVTEaq",
    "outputId": "8251f499-f230-46a2-e28d-00b3052f50f6"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6lvMNbpXLly"
   },
   "source": [
    "**Exercise 1c** Inspect the output of [get_prediction().summary_frame()](https://www.statsmodels.org/stable/generated/statsmodels.tsa.base.prediction.PredictionResults.summary_frame.html). Do you know what is in the `mean` and `mean_se` columns? Do you understand the difference `mean_ci` and `obs_ci` (*Mean Confidence Interval* and *Observation Prediction Interval*)?\n",
    "\n",
    "Use the summary frame data to compute the residuals $\\hat{\\epsilon}_i = Y_i - X_i\\hat{\\beta}$. Calculate the standardized residuals $(\\hat{\\epsilon}_i - \\text{mean}(\\hat{\\epsilon}))/\\text{sd}(\\hat{\\epsilon})$, and square roots of absolute values of standardised residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMHmUczy6IFL",
    "outputId": "415d0616-ef2a-4b17-972c-da27b4548be3"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjQM8k3oU94F"
   },
   "source": [
    "**Linear functional form**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3VABjrmVUq_"
   },
   "source": [
    "Based on the ground truth model that we used (we know how $y$ was generated), we know that the linear functional form assumption is satisfied.\n",
    "\n",
    "In practice we would use the [Ramsey RESET test](https://en.wikipedia.org/wiki/Ramsey_RESET_test) (Regression Specification Error Test) to check that the model's functional form is correct, with:\n",
    "\n",
    "$$H_0:X\\beta+\\varepsilon$$\n",
    "$$H_1:f(X\\beta)+\\varepsilon$$\n",
    "\n",
    "where f() is non-linear. Important: The functional form's linearity applies to the coefficients' powers; the x-variables can be transformed freely. There are several versions of this test. The version presented here estimates the original regression and obtains the fitted values. An auxiliary regression is then estimated in which successive powers of the fitted values from the primary regression become the independent variables. We test for joint insignificance of the estimated coefficients at powers greater than 1 of the fitted values in the auxiliary regression.\n",
    "\n",
    "Interestingly, a similar idea to the RESET test can be used to test the functional form in other models. A generalized version of this test is called the link test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTIx2h3hW-im",
    "outputId": "74fe12a9-0a81-4466-d626-133fd67f5e78"
   },
   "outputs": [],
   "source": [
    "# Results of RESET test\n",
    "print(sm.stats.diagnostic.linear_reset(results, power=3, test_type='fitted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe025iBcYsMS"
   },
   "source": [
    "The p-value in the RESET test is higher than any standard significance level, so we support the null hypothesis. Note that RESET is designed for small-sized samples -- if the sample is big enough, it tends to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L51gtvtZFI1"
   },
   "source": [
    "**Exercise 1d** Prepare the first type of diagnostic plots, i.e., plot the residual values vs the fitted values, e.g., using `px.scatter()`. If the assumptions are satisfied, you should see a cloud of points and no trend and the data would be evenly distributed along the horizontal line (add such a line to the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "fpGfLO3ZZwS5",
    "outputId": "ed523f7b-7079-4dbe-ac56-e124fb18ec9c"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okcriq8xAOVa"
   },
   "source": [
    "A useful tweak to this plot is to add a trend line visualizing the relationship between the fitted values and the residuals. This can be easily done with [seaborn.regplot](https://seaborn.pydata.org/generated/seaborn.regplot.html). If the assumptions about linearity are satisfied, you should see a line as close to a horizontal one as possible (minor deviations are acceptable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "WgAAYIkVAzbH",
    "outputId": "fb6f8e35-4021-40d7-d633-4f4e1a248c07"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model values\n",
    "model_fitted_y = results.fittedvalues\n",
    "# model residuals\n",
    "model_residuals = results.resid\n",
    "\n",
    "# here we use matplotlib\n",
    "# with sns.residplot\n",
    "# we draw the scatterplot of residuals against the fitted values (scatter=True)\n",
    "# and we add a regression line\n",
    "plot_lm_1 = plt.figure()\n",
    "plot_lm_1.axes[0] = sns.regplot(x=model_fitted_y, y=model_residuals,\n",
    "                                scatter=True,\n",
    "                                ci=False,\n",
    "                                lowess=True,\n",
    "                                line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "\n",
    "plot_lm_1.axes[0].set_title('Residuals vs Fitted')\n",
    "plot_lm_1.axes[0].set_xlabel('Fitted values')\n",
    "plot_lm_1.axes[0].set_ylabel('Residuals');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBIqS52zfkrd"
   },
   "source": [
    "**Assumptions about the random disturbance (error term)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mr1aw-LklLZ"
   },
   "source": [
    "We check the assumptions about the behavior of the distribution of the random disturbance (error term) based on the residuals. Note that we know the relationship between the residuals and the random component thanks to the idempotent matrix M -- the theory presented in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21joeeYDnCtW"
   },
   "source": [
    "***Expected value of the random disturbance***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEaKcVRxnK3k"
   },
   "source": [
    "If the model contains the constant term, there is no need to check the assumption about the expected value of the random disturbance being zero. Because of the properties of the regression hyperplane, the sum of the residuals is zero in the models with constant terms. The mean of the residuals will also be zero, so it is reasonable to use the mean as an estimate of the expected value in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uS6VamKk_qP"
   },
   "source": [
    "**Exercise 1e** Compare the mean of the residuals in the model with and without the constant. Due to the finite precision of the calculations, you will get a non-zero value in both cases, but this value will be much lower (in terms of absolute values) in the model with a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HH0mrP_olEIJ",
    "outputId": "b2b7f800-4f14-4f45-830a-9b799a3979d2"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeOSrYQwmo3T"
   },
   "source": [
    "***Normality of the distribution of the random disturbance***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTT_6DqSnjig"
   },
   "source": [
    "We can verify this (additional!) assumption with the graphical analysis of the distributions of the residuals. Graphical analysis of residuals can be done in several ways:\n",
    "\n",
    "* Histogram/kernel density plot (density plot)\n",
    "* Quantile-quantile chart\n",
    "* Boxplot (boxplot)\n",
    "\n",
    "\n",
    "As here we are working with a relatively large sample -- it is not surprising that we can expect that, under CLT, the distributions of statistics will converge to standard ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wldl2wMCn-0X"
   },
   "source": [
    "**Exercise 1f** Inspect the distributions of the residuals in your favorite way. For example, draw the histogram or a qq-plot (see [statsmodels.graphics.gofplots.qqplot](https://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqplot.html) and explanation [here](https://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html) and [here](https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot)) of the residuals. There should be no significant differences between the empirical and the trend line in the qq-plot, and the histogram should resemble the bell curve. Compare your insights with the conclusion from the Shapiro-Wilk test (e.g., from `scipy.stats`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "kKHocr06oTAx",
    "outputId": "7c5af0fd-9278-42c9-87b3-4c8db1a5874c"
   },
   "outputs": [],
   "source": [
    "# put your code here (histogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "grPxgS7OLPtN",
    "outputId": "031fbf01-6711-4bc4-a389-52fd48a8cc5b"
   },
   "outputs": [],
   "source": [
    "# put your code here (qq-plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmkDogdmLU72",
    "outputId": "d9c80bdc-4d46-4351-d0f8-731224aa79f0"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0QIjM6bqiUS"
   },
   "source": [
    "***Homoscedasticity and the lack of autocorrelation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quiHmqR-sWQF"
   },
   "source": [
    "Here, we will use two types of plots: (1) square root of standardized residuals vs the fitted values (also known as scale vs location) and (2) line plot with subsequent residual values.\n",
    "\n",
    "Let us start with the first type of plots for checking homoscedasticity and lack of autocorrelation: **scale vs location** plot. If the assumptions about the sphericity of the random disturbance are satisfied, we should see a cloud of data in the first graph. If you see triangular shapes of the data clouds, it indicates heteroscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyVW8Uto73-8"
   },
   "source": [
    "**Exercise 1g** Prepare the **scale vs location diagnostic plot**. Plot the squared standardized residuals against the fitted values. Compare with the insights from the first diagnostic plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "mDHPxqZIuSar",
    "outputId": "e2ac36a2-1909-42a9-db2b-a1effa79d941"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_kgBHJH70XX"
   },
   "source": [
    "However, the plot above may be tricky to interpret. That is why you may see a 'tweaked' version of this plot, as shown in the code below. We add a trend line (similarly to the first diagnostic plot) for visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "YrqqQffy0Nt9",
    "outputId": "ab79cedf-650d-4875-f9fd-aa6b89a2c837"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model values\n",
    "model_fitted_y = results.fittedvalues\n",
    "# model residuals\n",
    "model_residuals = results.resid\n",
    "# normalized residuals\n",
    "model_norm_residuals = results.get_influence().resid_studentized_internal\n",
    "# absolute squared normalized residuals\n",
    "model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n",
    "\n",
    "plot_lm_2 = plt.figure()\n",
    "sns.regplot(x=model_fitted_y, y=model_norm_residuals_abs_sqrt,\n",
    "            scatter=True,\n",
    "            ci=False,\n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\n",
    "plot_lm_2.axes[0].set_title('Scale-Location')\n",
    "plot_lm_2.axes[0].set_xlabel('Fitted values')\n",
    "plot_lm_2.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5X5HEiY6QKB"
   },
   "source": [
    " The more horizontal the red line is, the more likely the data is homoscedastic.\n",
    "If the trend line is horizontal or deviates only slightly, \"it's fine.\" In our case, we see a slight positive deviation between the low and high fit values. However, these are negligible fluctuations. In Exercise 2, you will be able to compare how the trendline behaves when problems occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRDtPibgDwLM"
   },
   "source": [
    "As for the second type of plot for checking homoscedasticity and lack of autocorrelation -- **the line plot with subsequent residual values** -- it allows you to inspect if there are (particularly) problems with autocorrelation. However, you can also spot heteroscedasticity there. If the assumptions about the sphericity of the random component are satisfied, you should see something that resembles a plot of a sound wave with a constant amplitude. The fluctuations should be around zero. Single stronger fluctuations are not a problem.\n",
    "\n",
    "The problems occur if, for example:\n",
    "- the amplitude of fluctuations increases (you see a funnel) -- the variance is not constant (heteroscedasticity),\n",
    "- you notice a trend -- there is autocorrelation,\n",
    "- periods of high amplitude alternate with periods of low -- unstable variance, \"variance clustering\" (heteroscedasticity),\n",
    "- the values do not oscillate around one value -- for example, there is an alternating trend, the graph resembles a sine wave (autocorrelation).\n",
    "\n",
    "The above list, of course, does not exhaust all possible problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1o4nN_QEwn2"
   },
   "source": [
    "**Exercise 1h** Prepare the another diagnostic plot: **the line plot with subsequent residual values**. For convenience, add a horizontal line through zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "AC4oxY5pE9DN",
    "outputId": "2a8ec423-b7a1-4657-f406-fae89db9f4ad"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GfVlYJNHBIq"
   },
   "source": [
    "**Lack of multicolinearity and influential observations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcxbfc_ZHm6_"
   },
   "source": [
    "The last thing -- we need to check if we have the problem of multicollinearity in our model and if there are influential observations. Typically, we say there is a problem if the VIF is about 10 (or the mean VIF is about 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzKkrLz5H6jJ",
    "outputId": "19820d26-3f4b-4f9a-e894-339e5508c7dd"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "\n",
    "#calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "                          for i in range(len(X.columns))]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NfhlpQcLDwI"
   },
   "source": [
    "**Exercise 1i** Interpret the value of VIF in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq3d0dfTMezJ"
   },
   "source": [
    "**Exercise 1j** Find out if there are influential observations (deleating them would noticeably change the model) in the model with `sm.graphics.influence_plot()`. You should be worried if you see the observations in the top right or bottom right part of the plot (this means those observations have, at the same time, high [leverages](https://en.wikipedia.org/wiki/Leverage_(statistics)) and standardized residuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "7dfyBJVuMtuD",
    "outputId": "799d1b01-9c9d-44ba-99a4-3b4902358502"
   },
   "outputs": [],
   "source": [
    "sm.graphics.influence_plot(results, criterion=\"cooks\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQAdYjQvLjKS"
   },
   "source": [
    "## Determining which assumptions are not satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXIT8u7yO36J"
   },
   "source": [
    "**Exercise 2.** Find out what problems (if any) exist in those models. $n$ is the number of observations, and $k$ is the number of the parameters in the model (including the constant term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsK4SdixR5y4"
   },
   "source": [
    "a) n = 204, k = 3\n",
    "<center><img src='https://mimuw.edu.pl/~dot/resources/wum/diag1.png'>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYRQ3NQyTFP6"
   },
   "source": [
    "b) n = 1000, k = 4\n",
    "<center><img src='https://mimuw.edu.pl/~dot/resources/wum/diag2.png'>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uV0CiY7U2I4"
   },
   "source": [
    "c) n = 1000, k = 4\n",
    "<center><img src='https://mimuw.edu.pl/~dot/resources/wum/diag3.png'>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53_P1NpnVuRn"
   },
   "source": [
    "d) n = 500, k = 4\n",
    "<center><img src='https://mimuw.edu.pl/~dot/resources/wum/diag4.png'>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDK0nt2NWmHV"
   },
   "source": [
    "e) n = 340, k = 2\n",
    "<center><img src='https://mimuw.edu.pl/~dot/resources/wum/diag5.png'>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTmLpMlcn-Jy"
   },
   "source": [
    "**Exercise 3 (homework)** In this exercise, we will predict the income of a municipality in 2020 based on its population and voivodeship.\n",
    "\n",
    "Create a dataframe with the territorial code, income, population, and voivodeships of municipalities in 2020 using `pd.merge()` to perform a join with the `Code` variable as the key. Remove rows with missing values.\n",
    "Use the `pd.get_dummies()` function to encode the voivodeship for each municipality with dummy variables.\n",
    "\n",
    "Estimate the model and inspect its summary.\n",
    "Are the variables jointly significant according to the F-test?\n",
    "Are all individual variables significant according to the t-test? What are the interpretations of the parameters?\n",
    "Can you use a model with intercept in this exercise? Why / why not? If yes, what is its interpretation?\n",
    "\n",
    "Conduct the diagnostics of the model. Decide which assumptions are satisfied to an appropriate degree.\n",
    "If you detect an outlying observation, remove it from the data set, rerun the calculations and diagnostics and check if it improves the model fit.\n",
    "\n",
    "If you detect heteroskedasticity (non-constant variance of residuals), transforming the data may help.\n",
    "You may transform both the dependent and independent variables.\n",
    "Transforming the latter changes the functional relationship between the variables (i.e., whether they are linearly related), while transforming the former changes both the relationship and the residual variance structure.\n",
    "\n",
    "Estimate the average error in PLN that you would make if you used your model to predict a municipality's income from its population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TZDH7NjG97om",
    "outputId": "9c66d60c-4ef8-49f5-c0d3-1c78d4383fbf"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELzZoCBtq8tc"
   },
   "source": [
    "## Multicollinearity and omitted variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjvrpWUx8qsM"
   },
   "source": [
    "**Exercise 4.**  In this exercise, we will show what may happen to our statistical reasoning and the estimates of the parameters if we have multicollinearity in our model or we did not include a significant variable. We will again work with artificial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uG1T6hBra2ML"
   },
   "source": [
    "**Exercise 4a** Generate the data ($n=1000$) in a way similar to the one shown in Exercise 1. Our dataset contains the following variables:\n",
    "\n",
    "- A, which is not correlated with the others, $\\mu=0, \\sigma^2=1$\n",
    "- B and C, which are highly correlated (covariance = 24.9875), both with $\\mu=20, \\sigma^2=25$\n",
    "- D that will be correlated with variable C (covariance = 8.0), and with B (covariance = 7.99), $\\mu=4, \\sigma^2=4$\n",
    "\n",
    "Use seed 23033023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "yUDl1KiqcCiS",
    "outputId": "9e3b0f79-a001-4b72-9c5a-dfeacec0aaa2"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qzQrCgPdwnt"
   },
   "source": [
    "**Exercise 4b** Create a variable $y$, which will be the dependent variable in the regression. $ y = 0.3*A + 0.5*B + 0.6*C + \\varepsilon$. This way, we will obtain a \"real\" model. Y will depend on A, B, and C (but not on D). We will also add a small random disturbance (from $N(0,1)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "fcojh8GCdxdg",
    "outputId": "f4726450-76f2-4525-b96a-7ab38ad96959"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiAIRzqGeM2L"
   },
   "source": [
    "**Exercise 4c** Inspect the descriptive statistics of the data set.\n",
    "Visualize the relationships between dependent and independent variables. If you had no knowledge about the true model -- which variables would you include?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "ekTM3lahenHH",
    "outputId": "8c684eb6-079e-4cec-b55d-35d43220d4a5"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xWO97uGi_tK"
   },
   "source": [
    "**Exercise 4d** Estimate the model and inspect its summary. Are the variables jointly significant according to the F-test? Are all individual variables significant according to the t-test? Compare the estimates against the true values of the parameters. Inspect VIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVjNUrZ1ntOZ",
    "outputId": "061a0229-77c1-4c60-a1cb-1ef16684b9cd"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwYjWL57tYNj"
   },
   "source": [
    "**Exercise 4e** Let us include variable D in the model, which we know should be statistically insignificant (it did not participate in creating variable $y$) but is strongly correlated with variable C (statistically significant). Inspect the summary and VIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukkMruWJt8Pj",
    "outputId": "ec7127e1-259d-46a1-ce64-c42958a1978b"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFuiV0i_xkLn"
   },
   "source": [
    "**Exercise 4f** Let us construct a model that only includes A, D, and a constant term. D is absent in the true model, and we omitted two significant variables (B and C). Inspect the summary of the model and VIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvJOpOQjyLwL",
    "outputId": "60e9c9be-460b-4150-ce71-1eb8a137dc7c"
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
