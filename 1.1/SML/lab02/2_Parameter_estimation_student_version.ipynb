{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "685bcedd"
   },
   "source": [
    "# Statistical machine learning - Notebook 2, version for students\n",
    "**Author: Michał Ciach, Grzegorz Preibisch, Dorota Celińska-Kopczyńska**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et5_2Neh7Sb1"
   },
   "source": [
    "In today's class, we will learn some aspects of parameter and interval estimation.\n",
    "In the first section, we will focus on the graphical analysis of the properties of the point estimator (e.g., mean value of a distribution) using artificial data.\n",
    "In the second section, we will focus on estimating the mean value of a distribution using a statistical sample.   \n",
    "In the last section, we'll calculate confidence intervals for the mean using real-world data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c1qctHRoY_V",
    "outputId": "d8217daa-18cc-441b-daed-b6670a1c7a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PA1wjZ4w_ovO",
    "outputId": "a856bf85-9ef5-4816-d98e-d92d33401fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xOJfD-jexDbHSOCg1EiyAxqc5kXjMvX0\n",
      "To: /content/protein_lengths.tsv\n",
      "\r  0% 0.00/29.3M [00:00<?, ?B/s]\r 59% 17.3M/29.3M [00:00<00:00, 151MB/s]\r100% 29.3M/29.3M [00:00<00:00, 183MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1xOJfD-jexDbHSOCg1EiyAxqc5kXjMvX0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnpa_rkDoWlo"
   },
   "source": [
    "## Data & library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UWS4VSq39w7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import plotly.express as px\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t as tstud\n",
    "from typing import List\n",
    "from typing import Callable\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Dh5IunLi7M0x",
    "outputId": "2c3e032f-af67-4942-bcc1-06ec11f7597c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-70712ffd-3e0b-49f6-9fc2-4e27c8593d62\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scientific name</th>\n",
       "      <th>Common name</th>\n",
       "      <th>Protein ID</th>\n",
       "      <th>Protein length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000005.3</td>\n",
       "      <td>1474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000006.2</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000007.1</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000008.1</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000009.1</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648731</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560453.1</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648732</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560454.1</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648733</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560455.1</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648734</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560456.1</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648735</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560457.1</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648736 rows × 4 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70712ffd-3e0b-49f6-9fc2-4e27c8593d62')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-70712ffd-3e0b-49f6-9fc2-4e27c8593d62 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-70712ffd-3e0b-49f6-9fc2-4e27c8593d62');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-cc458be6-13f1-47f8-8c3b-497f56691f31\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc458be6-13f1-47f8-8c3b-497f56691f31')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-cc458be6-13f1-47f8-8c3b-497f56691f31 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       Scientific name            Common name    Protein ID  Protein length\n",
       "0         Homo sapiens                  Human   NP_000005.3            1474\n",
       "1         Homo sapiens                  Human   NP_000006.2             290\n",
       "2         Homo sapiens                  Human   NP_000007.1             421\n",
       "3         Homo sapiens                  Human   NP_000008.1             412\n",
       "4         Homo sapiens                  Human   NP_000009.1             655\n",
       "...                ...                    ...           ...             ...\n",
       "648731   Imleria badia  Bay bolete (mushroom)  KAF8560453.1             494\n",
       "648732   Imleria badia  Bay bolete (mushroom)  KAF8560454.1             737\n",
       "648733   Imleria badia  Bay bolete (mushroom)  KAF8560455.1             554\n",
       "648734   Imleria badia  Bay bolete (mushroom)  KAF8560456.1             813\n",
       "648735   Imleria badia  Bay bolete (mushroom)  KAF8560457.1             102\n",
       "\n",
       "[648736 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_lengths = pd.read_csv('protein_lengths.tsv', sep='\\t')\n",
    "protein_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdIQlMXd2bly"
   },
   "source": [
    "## Point estimation -- simulational approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y14PycwLx8lX"
   },
   "source": [
    "During the lecture, we introduced the properties of the estimator. To develop more intuition about them, in this notebook we will graphically analyze their distributions (and the distributions of some relevant statistics).\n",
    "\n",
    "Contrary to other notebooks, in this one we will start with analyzing artificial data. The advantage of (pseudo)randomly generated data is that we are able to control nearly every aspect of the study. E.g., if we want to analyze the properties of the estimator, we need to know its true value (the groundtruth, something we know we should obtain). In real-world data we rarely know the true values. Using (pseudo)randomly generated data allows us to set the true values as the parameters for the generation mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP49P_Y2x8lY"
   },
   "source": [
    "**Exercise 1.** In this exercise we will analyze the distribution of a few estimators (for normally distributed data) from repeated experiments (the number of repetitions will be $N$). The size of the sample ($n$) will be the same in each experiment. To this end:\n",
    "\n",
    "* Write a function that will return $N$ samples of size $n$ from from gaussian distribution with a given mean $\\mu$ and variance $\\sigma^2$. Run the function with parameters: $N=1000$, $n = 30$, $\\mu = 200$, $\\sigma^2=144$\n",
    "* Based on the dataset from the previous point, for each sample of size $n$ compute the following estimators:\n",
    "\n",
    " - mean $\\hat{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$\n",
    " - unbiased estimator for variance $\\tilde{S}_n^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    " - biased estimator for variance $S_n^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    " - estimator for variance with mininal MSE $S_n^{2*} = \\frac{1}{n+1} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    "\n",
    "Hint: For that purpose, we suggest to write a function and change the functions for the estimators when necessary. For variance estimators, make the fullest use of `np.var`.\n",
    "\n",
    "* Create the histograms for the values of the computed estimators. Plot the values of the true parameters ($\\mu = 200$, $\\sigma^2=144$ ). What can you say about the distribution of the estimators? Are there differences in the shape of the distributions of the estimators? Discuss and justify your view.\n",
    "* Compute the biases, variances, and MSE of the estimators. Do the results agree with the theoretical results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBSbfzZtzSmt"
   },
   "source": [
    "**Exercise 2. (optional)**  In the previous exercise, we worked with the distribution of the estimators by running multiple experiments with a given sample size (by default quite small). In this exercise, we will analyze the asymptotic properties of the estimators. To this end:\n",
    "\n",
    "* Compute 100 samples of each size from 2 to 5000 (you may keep using data from N(200,144)). For each sample compute:\n",
    "\n",
    " - $\\hat{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ as a mean estimator\n",
    " - $\\hat{X} +10$ as a mean estimator\n",
    " - unbiased estimator for variance $\\tilde{S}_n^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    " - biased estimator for variance $S_n^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    "\n",
    "* Scatterplot the obtained values of the parameters against the sample size: y axis should the the value of the parameter, and x axis should be the size of the sample. Add a horizontal line with the true value of the parameter. Compare the plots. Do values of the estimators become closer to real values when the size of the sample increases? Do the results agree with the results on consistency of the estimators? Discuss.\n",
    "\n",
    "* Compute the biases of the estimators. Plot the biases of the estimators against the sample size. Add a horizontal line in zero. Compare the insights from the plots with the theory of asymptotical unbiasedness.\n",
    "\n",
    "Hint: We encourage you to use scatterplots, but visualizations of distributions may also come in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7bLoYLTz9CH"
   },
   "source": [
    "**Exercise 3 -- optional homework.** Find an MLE estimator for $\\lambda$ in exponential distribution (probability density function  $f(x) = \\lambda \\exp(-\\lambda x)$ for $x \\in [0, \\infty)$), using pen and paper.\n",
    "Prepare a similar analysis of the properties of the found estimator as the analyses in Exercise 1 and 2 (analysis of the distribution with a given sample size and the analysis of the asymptotical properties based on the increasing sample sizes). You may assume true $\\lambda = 2$. Discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV4zsbLpx8lg"
   },
   "source": [
    "## Exploratory analysis\n",
    "The first step to any statistical analysis is to explore the data - check the basic statistics like the mean and variance, and visualize the data to see what kind of distribution we're dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPeeEe5tvyOB"
   },
   "source": [
    "**Exercise 4.** In this exercise, we'll extract the data about human proteins, perform a simple data transformation, and do a basic exploratory analysis.\n",
    "\n",
    "1. Select the data about human protein lengths from the `protein_lengths` data frame, and put it into a data frame `human_protein_lengths`. Here, you may need to use the `.copy()` method for the subsequent steps to work (ask your tutor if you need a further explanation).\n",
    "2. Calculate the base-10 logarithm of the protein length and append it to the `human_protein_lengths` data frame as a column called `LogLength`.\n",
    "3. Use the `human_protein_lengths.describe()` method to check the basic statistics of the numerical columns of the data frame. What is the average length of a human protein? What is the maximum length?  \n",
    "4. Use the `px.histogram()` functions to create histograms showing the distributions of the protein lengths and log-lengths. Which distribution is more spread around its average? Does any distribution resemble the Normal (Gaussian) distribution? Are there many proteins with lengths similar to the maximum length, or just a few?  \n",
    "5. Calculate the average length and log-length and their standard deviations; Store them in variables `true_mean`, `true_mean_log`, `true_std` and `true_std_log`. We'll use them in subsequent exercises as our *ground truth* against which we'll evaluate our estimators.     \n",
    "\n",
    "*Quick question*. Is $\\text{true_mean}$ equal to $10^\\text{true_mean_log}$? Why/why not? (note that we've used the base-10 logarithm)\n",
    "\n",
    "*Why the base-10 logarithm?* Mathematicians usually prefer to use the natural logarithms. In statistical data analysis, we sometimes use also the base-10 logarithms, because their values are easier to interpret as orders of magnitude (or simply the numbers of digits) of our values. Although the logarithms are mostly equivalent mathematically, an easier interpretation is important to get more meaningful conclusions from the data.\n",
    "\n",
    "*Why the standard deviation?* Some students may wonder why we prefer the standard deviation rather than the variance - after all, the difference is just a square root, so mathematically it's almost the same thing. Here, again, the reason is the interpretability of the results. When we compute the variance, we square the observations. As a consequence, their units also get squared. This means that, if we estimate the number of mushrooms in a forest, the variance is expressed in terms of *mushrooms squared*, which doesn't make any sense. Taking a square root brings the unit back to mushrooms.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmcN4rsybZqV"
   },
   "source": [
    "## Point estimation\n",
    "\n",
    "One of the main strengths of statistical theory is that it allows us to estimate many quantities (like the mean protein length, mean income in a country, or voting preferences)  using only a sample of randomly selected observations, and, most importantly, to estimate the uncertainty of such estimation. This is the main reason why we derive properties of estimators, such as their expected value and variance. Good statisticians can derive estimators which need less observations and give better results.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjVsjUljgcXy"
   },
   "source": [
    "**Exercise 5.** In this exercise, we'll do an empirical analysis of the properties of the estimator of the mean. We'll use a sample of $N=1000$ randomly selected human proteins. Denote $X_i$ as the length of a randomly selected human protein, and $\\log(X_i)$ as its base-10 logarithm. Define the following two estimators:\n",
    "\n",
    "$$\\hat{\\mu}_X = \\sum_{i=1}^N X_i/N, \\text{an estimator of the mean length}$$\n",
    "\n",
    "$$\\hat{\\mu}_{\\log(X)} = \\sum_{i=1}^N \\log(X_i)/N, \\text{an estimator of the mean log-length}$$  \n",
    "\n",
    "First, we'll draw $R=2000$ independent samples and calculate the estimators. Here's an example way to do this:   \n",
    "\n",
    "1. Create empty lists called e.g. `means` and `means_log`.  \n",
    "2. Repeat the following $R$ times (e.g. using a `for` loop):  \n",
    "    2.1. Get a random sample of size $N$ of the observations (i.e. rows) from `human_protein_lengths`; you can use the `.sample()` method.   \n",
    "    2.2. Calculate the mean length and append to `means`  \n",
    "    2.3. Calculate the mean log-length and append to `means_log`.     \n",
    "3. Convert both lists to `numpy` arrays (e.g. `means = np.array(means)`)\n",
    "\n",
    "Now, we can inspect how well the estimators approximate the *true* mean length $\\mu_X$ and the *true* mean log-length $\\mu_{\\log{X}}$ (notice the lack of hats above $\\mu$'s - this means that these are the true parameters, not estimators).\n",
    "\n",
    "4. Estimate the mean value of the estimator of the mean (by running `np.mean(means)`). Is it close to the true value $\\mu$? In other words, does the estimator seem *unbiased*?\n",
    "5. Estimate the bias of the estimator of the mean log-length (using the values in `means_log`). Does it seem biased? Does the result agree with the theoretical one about the estimator of the mean?       \n",
    "5. Estimate the Root Mean Square Error of the estimator of the mean, defined as $\\text{RMSE}(\\hat{\\mu}_X) = \\sqrt{\\mathbb{E}(\\hat{\\mu}_X - \\mu_X)^2}$. This will tell you, approximately, the average error of $\\hat{\\mu}_X$ in terms of the number of amino acids (the building blocks of proteins). Why did I write *approximately*? (Hint: it's not just becasue we estimate it rather than calculate it theoretically)\n",
    "6. Estimate $\\text{RMSE}(\\hat{\\mu}_{\\log(X)})$. How can you interpret the result?\n",
    "7. Estimate the standard deviations of the estimators. Which one is less variable? Does it mean that one quantity is easier to estimate than the other?\n",
    "8. Is $\\text{sd}(\\hat{\\mu}_X) = \\text{RMSE}(\\hat{\\mu}_X)$? Why/why not? Is the equation always true, sometimes true, or never true?    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQIElPvrKT7o"
   },
   "source": [
    "**Exercise 6.** The standard deviation of an unbiased estimator tells us how much it fluctuates around the true value. An estimator with a lower standard deviation will more often give us values that are close to the true one. This way, we can compare two estimators of the same thing.  \n",
    "\n",
    "However, the standard deviation is often not that useful when comparing the measurements of two different things. This is because it depends on the units of the measurement. Suppose we measure the length $L$ of some objects in meters, and the standard deviation of the measurement is $\\text{sd}(L)$. Measuring the same object in centimeters will give us a measurement equal $100L$, and the corresponding standard deviation $\\text{sd}(100L) = 100\\text{sd}(L)$ will appear to be much larger, but it doesn't mean that measurements in centimeters are more difficult. To make matters worse, in real-life applications, the variability of the measurement often depends on its average value, regardless of the units. The standard deviation of the height of a mouse (a few milimeters) is much lower than the one of an elephant (several centimeters), but it doesn't mean that mice are easier to measure. Similarly, in the case of protein length and log-length, the latter is much smaller, so it can be expected that its standard deviation will be smaller as well.\n",
    "\n",
    "To evaluate the variability of an estimator regardless of its units and the average value, we can calculate a so-called [*coefficient of variation*](https://en.wikipedia.org/wiki/Coefficient_of_variation) (variation, not variance!). For a random value $Y$, this is defined as $\\text{cv}(Y) = \\text{sd}(Y)/\\mathbb{E}(Y)$.   \n",
    "\n",
    "7. Calculate the coefficients of variation for the estimators of mean protein length, i.e. $\\text{cv}(\\hat{\\mu}_X)$, and log-length, i.e. $\\text{cv}(\\hat{\\mu}_{\\log(X)})$. Which estimator is better in this case? In general, does a lower coefficient of variation always mean that an estimator is better?\n",
    "8. Is $\\text{cv}(Y)$ always equal to  $\\text{sd}(Y/\\mathbb{E}(Y))$? Is there a condition for $Y$ that makes it equal? Give an analytical argument and verify that empirically on the protein length data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMiA2fX8KehP"
   },
   "source": [
    "**Exercise 7.** Remember that the estimators are random variables with their own distributions! We can explore their distributions visually.  \n",
    "\n",
    "4. Use the `px.histogram()` function to generate histograms of the estimator of the mean $\\hat{\\mu}_X$ and the log-mean $\\hat{\\mu}_{\\log(X)}$.\n",
    "5. Annotate the histograms with the true values $\\mu_X$ and $\\mu_{\\log{X}}$ that you have computed in Exercise 1. You can do the annotation any way you want, I typically use a red dot at the bottom of the histogram or a vertical line. Are the estimators centered around the true values? Which estimator is more focused (i.e. less spread) around the true value?\n",
    "6. Annotate the histograms with the average values of the estimators that you have computed in Exercise 3. Use different colors than in the previous point.\n",
    "7. Is the distribution of the estimator $\\hat{\\mu}_{\\log(X)}$ similar to the distribution of the protein log-lengths that you visualized in Exercise 1?\n",
    "6. Is the distribution of the estimator $\\hat{\\mu}_X$ similar to the distribution of the protein lengths that you visualized in Exercise 1? Or maybe it's more similar to the normal distribution now? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teytxf93C2Xa"
   },
   "source": [
    "**Exercise 8.** We've learned how to analyze the properties of an estimator for a fixed sample size $N$, and we can use this knowledge to do something even more useful: determining how many observations we need for an estimation with a given precision.\n",
    "\n",
    "1. Using the equations for the expected value and the standard deviation of the estimator of the mean, $\\hat{\\mu}_X = \\sum_{i=1}^N X_i/N$ where $X_i$ is the length of a randomly selected protein, calculate the sample size $N^*$ that we need to take in order for the standard deviation of the estimator to be equal to a fraction $p$ of the true mean (i.e. $\\sigma_\\hat{\\mu} = p\\mu$; note the lack of a hat above sigma - it's not a random variable, but a parameter of the estimator). Express it in terms of coefficients of variability. You will need to assume that the standard deviation of the protein length is known; use the value in the `true_std` variable from Exercise 1.\n",
    "2. Calculate $N^*$ for the estimator of the average log-length (use the true standard deviation of protein log-lengths in the `true_std_log` variable). Is there a noticeable difference compared to the estimator of the average length? Which quantity is easier to estimate?   \n",
    "2. Analyze one of the estimators for a sample of size of the corresponding $N^*$: visualize its distribution, estimate its bias, standard deviation, coefficient of variation and RMSE. You can simply modify the code from the previous exercises.   \n",
    "  2.1 Use the results to verify if your calculation of $N^*$ was correct.  \n",
    "  2.2 How did the distribution of the estimator change compared to the previous sample size ($N = 1000$)?  \n",
    "3. *Quick question 1*. Does $N^*$ depend on the number of proteins that humans have? In order to get the same precision of the estimation (measured in terms of the RMSE), would you need a larger sample size if humans had a million proteins?  \n",
    "4. *Quick question 2.* Does $N^*$ depend on the distribution of the data?\n",
    "\n",
    "\n",
    "*Note.* In practice, the estimation of $N^*$ is often more difficult, because we usually don't know the true standard deviation; instead, we need to estimate it and take into account the error of this estimation when deriving $N^*$. Because of this, the required sample size will typically be larger than in the case of a known standard deviation. This topic is too complex to cover in this course, so we'll focus on the simpler case with a known standard deviation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po_VU4BPE8E1"
   },
   "source": [
    "**Exercise 9.** Can we use the estimated average log-length to estimate the average length of human proteins?  \n",
    "\n",
    "1. Consider a statistic given by the equation $\\hat{\\zeta} = 10^{\\hat{\\mu}_{\\log(X)}}$. Is it an estimator of the average income?  \n",
    "  1.1 Does $\\hat{\\zeta}$ correspond to some well-known mathematical object, e.g. some kind of mean?\n",
    "2. Regardless of the answer, let's try to use $\\hat{\\zeta}$ to estimate the average protein length. Use the randomly sampled values of $\\hat{\\mu}_{\\log(X)}$ from the previous exercises to calculate the corresponding values of $\\hat{\\zeta}$ and to estimate this estimator's expected value and standard deviation. You can use a sample size of your choice; try to compare the results for different sample sizes, like $N = 10, 100, 1000$.  \n",
    "3. Based on the results, do you think that $\\hat{\\zeta}$ is an unbiased estimator of the average length? Try to confirm your expectations by deriving formulas for the expected value of $\\hat{\\zeta}$.  \n",
    "  3.1.\\* If $\\hat{\\zeta}$ is biased, then how does the bias scale with the number of observations? Check this either theoretically by analyzing equations or empirically by estimating the bias for different sample sizes (e.g. create a plot showing the estimated bias depending on the sample size).   \n",
    "4. Does $\\hat{\\zeta}$ have a lower or a higher variance than $\\hat{\\mu}_{X}$? What about the coefficient of variability?   \n",
    "5. Plot the values of the estimators $\\hat{\\zeta}$ and $\\hat{\\mu}_I$ on two boxplots side-by-side and annotate it with the true average income (for example, using a horizontal line). Which estimator seems better?  \n",
    "6. Which estimator has a lower RMSE: $\\hat{\\zeta}$ or $\\hat{\\mu}_X$? Why?  \n",
    "7.\\* Do we have $\\text{RMSE}(\\hat{\\zeta}) \\geq \\text{RMSE}(\\hat{\\mu}_X)$ for all sample sizes? If not, then try to characterize the sample sizes for which  $\\hat{\\zeta}$ works better than $\\hat{\\mu}_X$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsrcrt2_SR5H"
   },
   "source": [
    "## Interval estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuDDXuOyqPKg"
   },
   "source": [
    "In the previous section, we've learned how to quantify and analyze the uncertainty of an estimation by analyzing the standard deviation of the estimator. In this section, we will learn a different technique - the estimation of *confidence intervals*, i.e. intervals which are likely to contain the true value of the parameter of interest.\n",
    "\n",
    "In general, we say that a confidence interval $[A, B]$ for a parameter $\\theta$ has a confidence level $\\alpha$ if it contains the true value of the parameter $\\theta$ with probability $\\alpha$:\n",
    "\n",
    "$$\\mathbb{P}(A \\leq \\theta \\leq B) = \\alpha$$\n",
    "\n",
    "Above, $A$ and $B$ are random variables calculated from the data (i.e. $A$ and $B$ are *statistics*). Note: some authors use a different terminology and would call this a level $1-\\alpha$; some authors also use a more general definition with $\\mathbb{P} \\geq \\alpha$ instead of $\\mathbb{P} = \\alpha$, because we often can't determine the exact probability and can only give its lower bound (you've seen this in the lecture with the Chebyshev confidence intervals).\n",
    "\n",
    "In principle, we can construct confidence intervals for any parameter of any distribution (e.g. the expected value, the variance, the proportion in the Bernoulli distribution, the shape parameter of the Gamma distribution etc.), but this is often difficult in practice. We'll focus on confidence intervals for the true mean (i.e. the expected value) of a normally distributed population - this is one of the most commonly used and one of the most useful confidence intervals.\n",
    "\n",
    "For the expected value of a normally distributed random variable, there are two commonly used confidence intervals: the conficence interval for a *known* $\\sigma$, given by the equation\n",
    "\n",
    "$$\\left (\\hat{\\mu} - q_{(1+\\alpha)/2}\\frac{\\sigma}{\\sqrt{N}},\\quad \\hat{\\mu} + q_{(1+\\alpha)/2}\\frac{\\sigma}{\\sqrt{N}} \\right ), $$\n",
    "where $q_{(1+\\alpha)/2}$ is the quantile of the standard normal distribution at the level of $(1+\\alpha)/2$; and the confidence interval for an *unknown* $\\sigma$, given by the equation\n",
    "\n",
    "$$\\left (\\hat{\\mu} - t_{(1+\\alpha)/2, N-1}\\frac{\\hat{\\sigma}}{\\sqrt{N}},\\quad \\hat{\\mu} + t_{(1+\\alpha)/2, N-1}\\frac{\\hat{\\sigma}}{\\sqrt{N}} \\right ), $$\n",
    "where $t{(1+\\alpha)/2, N-1}$ is the quantile of the Student's $t$ distribution with $N-1$ degrees of freedom at the level of $(1+\\alpha)/2$, and $\\hat{\\sigma}$ is the square root of the **unbiased** estimator of the variance, i.e. $\\hat{\\sigma} = \\sqrt{\\sum_{i=1}^N (X_i - \\bar{X})^2/(N-1)}$, where $\\bar{X} = \\hat{\\mu} = \\sum_{i=1}^N X_i$.\n",
    "\n",
    "If we simply plug $\\hat{\\sigma}$ instead of $\\sigma$ in the first kind of the confidence interval (the one for a known $\\sigma$), we get a third type of a confidence interval, a so-called *asymptotic confidence interval* for the mean; the name *asymptotic* comes from the fact that $\\hat{\\sigma} → \\sigma$ as $N → ∞$. As a consequence of this convergence, the asymptotic confidence interval gives quite accurate results for large sample sizes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtHFq07aZ2-f"
   },
   "source": [
    "**Exercise 10.** In this exercise, we'll do an empirical comparison of the properties of the three types of confidence intervals using the protein log-length data. We'll sample $R$ samples of some size $N$, calculate the corresponding confidence intervals for the mean, and check whether they have the desired confidence level and compare their lengths.   \n",
    "\n",
    "First, we'll prepare our data.    \n",
    "1. Create empty lists (or `numpy arrays`) that will contain the information whether the true mean is within a confidence interval (e.g. `within_normal` for the confidence interval with a known $\\sigma$, `within_student` for the confidence interval with an unknown $\\sigma$, `witin_asymptotic` for the asymptotic confidence interval).   \n",
    "2. Create empty lists (or `numpy arrays`) that will contain the lengths of the intervals.    \n",
    "3. Repeat the following $R=1000$ times (or more):  \n",
    "  3.1. Select a random sample of size $N$ of protein log-lengths; select $N$ of your choice.   \n",
    "  3.2. Calculate the three confidence intervals on the confidence level 95%. For the quantiles, you can use the `norm.ppf` and `t.ppf` functions from the `scipy.stats` package. For the normal confidence interval, use the known standard deviation in `true_mean_log`. Pay attention to the type of the estimator of standard deviation that you use! Some packages use the unbiased estimator of the variance, some don't!  \n",
    "  3.3. Calculate the lengths of the confidence intevals and append them to the corresponding lists.\n",
    "  3.4. Check whether the confidence intervals contain the true average log-length $\\mu_{\\log(X)}$, append the information to the corresponding lists.  \n",
    "\n",
    "Now, we'll use the generated data to analyze the properties of the confidence intervals.\n",
    "\n",
    "4. For each type of the confidence interval, estimate the probability that it contains $\\mu_{\\log(X)}$. Is the estimated probability close to the desired confidence level for each type? Why/why not? Does the answer depend on $N$?  \n",
    "5. Calculate the average length of each type of the confidence interval. Which type tends to give the shortest intervals? Which type tends to give the longest? Why?    \n",
    "6. Plot histograms depicting the distribution of the lengths of the confidence intervals.\n",
    "7. What are the advantages and disadvantages of each type of the confidence interval? Does the asymptotic confidence interval have any advantages over the other two?   \n",
    "8. *Quick question.* For a single sample, do all of the three types of confidence intervals always contain $\\hat{\\mu}_{\\log(X)}$?  \n",
    "\n",
    "Repeat this exericise using the protein lengths instead of log-lengths. What went wrong and why? Does the answer depend on $N$?      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fSF9sIaU2O0"
   },
   "source": [
    "**Exercise 11.** The length of the confidence interval is another (and more common) way to determine the required sample size. In this exercise, we'll see how to use it. As in exercise 6, we'll focus on the case of a known standard deviation.  \n",
    "\n",
    "1. Using the formula for the confidence interval with a known standard deviation, derive a formula for a necessary sample size $N^*$ such that the length of the confidence interval is at most some value $l$.  \n",
    "2. Calculate the required sample size for the average log-length for $l = 0.3$ (approximately 10% of the true mean).   \n",
    "3. Select a sample of the size calculated in the previous point and calculate an example confidence interval. Check if its length really is at most $l$.  \n",
    "  3.1. *Quick question 1.* Is the length of this confidence interval a random variable? Does it depend on the random sample?     \n",
    "4. *Quick question 2.* Do your calculations work for protein lengths as well?  \n",
    "5. Take a look at the formula for the confidence interval for an unknown standard deviation. Can you see why it may be difficult to derive a formula for $N^*$ in this case?  \n",
    "  5.1.\\* How would you approach calculating $N^*$ in this case?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nq5XFujPElGR"
   },
   "source": [
    "**Exercise 12.\\*** In this exercise, we'll see how the three types of the confidence intervals for the mean change with the sample size $N$. We'll also see some more details about their variability. We'll work on the protein log-length data.     \n",
    "\n",
    "1. Create variables to store the lower and upper bounds of each of the three types of the confidence intervals for the mean on a selected confidence level.\n",
    "2. For each value of $N$ from 3 to 100:  \n",
    "  2.1. Draw a random sample of size $N$.  \n",
    "  2.2. Calculate the three confidence intervals and store their bounds.  \n",
    "3. Create line plots with $N$ on the x axis and with the lower and upper bounds of the confidence intervals on the y axis.  \n",
    "  3.1.\\* For best results, try to create a single plot that shows all three types of confidence intervals. Make sure that each type of a confidence interval gets assigned a single, distinct color. To do this, you may need to import additional modules from `plotly`.    \n",
    "4. Annotate the plots with a horizontal line corresponding to the true mean.  \n",
    "5. Based on the plots, can you describe how the length of the confidence intervals changes with an increasing sample size? (this question is about the *trend*, i.e. the average change)    \n",
    "6. Which type of confidence intervals has the most variability, and which has the least? Why? (this question is about the variability *around the trend*, i.e. on top of the average change)   \n",
    "7. Where can you observe the largest differences between the three types of the confidence intervals?  \n",
    "8. Can you see how all the three types become more and more equivalent as the sample size grows? Why does that happen?  \n",
    "9. Does increasing the sample size always result in the same increase in the precision of the estimation?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLUb2C54fjS9"
   },
   "source": [
    "**Exercise 13.** A researcher has measured the blood glucose concentration (BGC) on a well-selected sample of 1000 individuals. The researcher has determined that BGC is normally distributed and that the 95% confidence interval for the average GBC is [192 mg/dl, 203 mg/dl]. Next, the researcher has run the following further studies. Evaluate whether they are correct, and if not, try to suggest corrections. Support your claims with analytical proofs or numerical simulations.\n",
    "\n",
    "- The researcher has tested a new, experimental methodology for measuring BGC on a well-selected selected sample of another 1000 individuals. The average BGC measured this way turned out to be 205 mg/dl. The researcher has concluded that the new methodology is biased.  \n",
    "- The researcher has tested a randomly selected group of 100 people who visited fast-food restaurants more than 5 times in the previous month. The confidence interval for BGC turned out to be [205 mg/dl, 210 mg/dl]. The researcher has concluded that frequently eating fast food increases the blood glucose concentration.  \n",
    "- The researcher has measured BGC on another set of 1000 individuals. The 95% confidence interval for this group was equal [198 mg/dl, 208 mg/dl]. The researcher has concluded that the average BGC in the whole population is most likely within [198 mg/dl, 203 mg/dl].   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
