{"cells":[{"cell_type":"markdown","metadata":{"id":"Wq4OjTS7NwK-"},"source":["## Project 1\n","# Zero-Shot Question Answering\n","The aim of this project is to get familiar with Language Models for Zero-Shot Question Answering and possible pitfalls when it comes to mesuring LLM performance on common benchmarks.  \n","The project is divided into two parts:\n","1. **Encoder Models**:\n","    - Here you will see how to used predefined HF / transformers classes to solve this task\n","2. **Decoder Models**:\n","    - Here you will see how to adapt a decoder model to solve this task and how are modern LLMs benchmarked on this task.\n","\n","### What is Zero-Shot Question Answering?\n","Zero-shot question answering is a task where a model is given a question and a context, and the model is expected to predict the answer without any training on the context or the question. The model is expected to generalize to unseen context and questions. From practical perspective it is a situation where we want to use a model to our task without any fine-tuning."]},{"cell_type":"markdown","metadata":{"id":"fIsuYUxQNwLA"},"source":["### Part 0: Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qt8nRNCzNwLB"},"outputs":[],"source":["%pip install datasets\n","%pip install 'transformers[torch]'"]},{"cell_type":"markdown","metadata":{"id":"GuajMlW3NwLC"},"source":["### Part 1: Dataset\n","\n","We will work on [MMLU dataset](https://huggingface.co/datasets/CohereForAI/Global-MMLU). Let's have a look at examples from the dataset. For each question we are given 4 answers, the correct one and the subject of the question"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JY0tz1HoNwLC","outputId":"61edfed2-b566-47de-ed89-cbb4fd013afe"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/barteksadlej/others/UW/NLP2025/project1/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import load_dataset, Dataset\n","\n","ds = load_dataset(\"CohereForAI/Global-MMLU\", \"en\", split=\"test\")\n","\n","def preprocess(sample: dict):\n","    return {\n","        \"options\": [\n","            sample[option]\n","            for option in [\"option_a\", \"option_b\", \"option_c\", \"option_d\"]\n","        ],\n","    }\n","\n","ds = ds.map(preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6KQ5YgFNwLD","outputId":"23203dc5-597f-4bde-c12f-f953e8af690f"},"outputs":[{"name":"stdout","output_type":"stream","text":["N Examples: 14042\n","Mean length: 274.54\n","Max length: 4671\n"]}],"source":["print(f\"N Examples: {len(ds)}\")\n","print(f\"Mean length: {sum(len(x['question']) for x in ds) / len(ds):4.2f}\")\n","print(f\"Max length: {max(len(x['question']) for x in ds)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ib3z1nYmNwLD","outputId":"9a67381a-71c3-41cd-b221-009ec4ebab28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n","Sample subject: abstract_algebra\n","Options:\n"," A: 0\n","B: 4\n","C: 2\n","D: 6\n","Answer: B\n"]}],"source":["sample_idx = 0\n","\n","sample_question = ds[sample_idx][\"question\"]\n","sample_subject = ds[sample_idx][\"subject\"]\n","options = ds[sample_idx][\"options\"]\n","answer = ds[sample_idx][\"answer\"]\n","\n","print(\"Sample question:\", sample_question)\n","print(\"Sample subject:\", sample_subject)\n","print(\"Options:\\n\", \"\\n\".join([f\"{c.upper()}: {o}\" for c, o in zip(\"abcd\", options)]))\n","print(\"Answer:\", answer)"]},{"cell_type":"markdown","metadata":{"id":"R2E1t7b9NwLE"},"source":["### Part 2: Encoder Models\n","\n","Let's have a look how to use out of the box transformers pipeline to solve this task"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bYz1dJ9NwLE","outputId":"124a266a-ee51-4690-9ed4-7446228344aa"},"outputs":[{"name":"stderr","output_type":"stream","text":["Device set to use mps:0\n"]}],"source":["from transformers import pipeline, set_seed\n","\n","set_seed(42)\n","\n","zero_shot_classifier = pipeline(\n","    \"zero-shot-classification\", model=\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THHido-rNwLE","outputId":"bd3b3e6b-8191-481e-dce9-fb81b07f3907"},"outputs":[{"data":{"text/plain":["{'sequence': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.',\n"," 'labels': ['6', '4', '2', '0'],\n"," 'scores': [0.27904871106147766,\n","  0.275924950838089,\n","  0.23752973973751068,\n","  0.20749661326408386]}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["zero_shot_classifier(\n","    sample_question,\n","    options,\n","    hypothesis_template=\"The correct answer is: {}\",\n","    multi_label=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"8mDESO1FNwLF"},"source":["#### How it works under the hood?\n","\n","If you go to the [source code](https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src/transformers/pipelines/zero_shot_classification.py#L49) you can see that it uses `ModelForSequenceClassification` and in [model card](https://huggingface.co/MoritzLaurer/ModernBERT-large-zeroshot-v2.0) you can read that the model was in fact fine tuned on question answering task.  \n","The base model used for fine-tuning is ModernBERT, which is a modernized version of the BERT model, making use of various advancements in the *atention* mechanism, improving both performance and efficiency.  \n","If you are interested in details, we highly recommend the following [Hugging Face blogpost](https://huggingface.co/blog/modernbert).\n","\n","By digging deeper in [model config](https://huggingface.co/MoritzLaurer/ModernBERT-large-zeroshot-v2.0/blob/a51e07b524299e309dd2b88d48b0cfa2bd9ec598/config.json#L24) we can see that the only labels the model knows about are\n","```\n","\"id2label\": {\n","    \"0\": \"entailment\",\n","    \"1\": \"not_entailment\"\n","  }\n","```\n","\n","For each option the model classifies the text\n","```\n","{question}\n","{hypothesis_template} {option}\n","```\n","as either entailment or not entailment. The option with the highest entailment score is the answer."]},{"cell_type":"markdown","metadata":{"id":"dbxpBtZSNwLF"},"source":["#### Task: evaluate the model on the dataset\n","Your task is to evaluate the model on the dataset and calculate some metrics (accuracy, potentially some other metrics and more granular insight - e.g. per question subject).  \n","Additionally you will implement batching to improve the evaluation performance and use profiler to analyze the improvements.\n","\n","Note that our problem is not typical classification task because the classes (here: available answers) are different for each question.  \n","The \"zero-shot-classification\" pipeline expects that the *classes* passed to it are the same for all examples in the batch.  \n","To overcome this limitation we need to reimplement the pipeline.\n","\n","The task involves the following steps:\n","\n","    1. First, implement a naive function which given the dataset (or its subset) processes it row by row using the zero-shot pipeline. (1 pkt)\n","    2. Implement a vectorized (batched) version of the pipeline. (4 pkt)\n","    3. Write a test function comparing the results of batching with the naive version. (1 pkt)\n","    4. Profile the batched version and (adaptively) choose the best batch size for processing the whole dataset. (2 pkt)\n","    5. Calculate accuracy of the model and some more insight on the results. (2pkt)\n","        Batching is not strictly required for this part."]},{"cell_type":"markdown","metadata":{"id":"upTd20U_NwLF"},"source":["#### Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzNglZxvNwLF"},"outputs":[],"source":["import gc\n","from textwrap import dedent\n","import torch\n","\n","\n","QUESTION_TEMPLATE = dedent(\n","    \"\"\"\n","    Question: {question}\n","\"\"\"\n",")\n","HYPOTHESIS_TEMPLATE = \"The correct answer is: {}\"\n","\n","\n","def flush():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    torch.cuda.reset_peak_memory_stats()"]},{"cell_type":"markdown","metadata":{"id":"IukVvAFdNwLF"},"source":["#### Naive implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ipOF0loWNwLF"},"outputs":[],"source":["from typing import TypedDict\n","from tqdm.auto import tqdm\n","\n","\n","class PipelineResult(TypedDict):\n","    labels: list[list[str]] #sorted according to scores\n","    scores: list[list[float]] #sorted descending\n","    top_inds: list[list[int]] #for each label, its index in the input's options list\n","\n","\n","def naive_zero_shot_classifier_pipeline(zero_shot_classifier, pipeline_input: Dataset) -> PipelineResult:\n","    \"\"\"A naive ZeroShotClassificationPipeline which iterates over examples and processes them one by one.\"\"\"\n","\n","    # TODO: Your code here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XSC9oBqNwLF"},"outputs":[],"source":["%%time\n","r = naive_zero_shot_classifier_pipeline(\n","    zero_shot_classifier, ds.take(256)\n",")"]},{"cell_type":"markdown","metadata":{"id":"jfG0cp3RNwLG"},"source":["#### Batched implementation\n","Rewrite the pipeline to process the dataset in batches to improve efficiency.  \n","ModernBERT supports a special batching mode called *sequence packing* but its usage requires FlashAttention and is beyond the scope of this task.  \n","Your goal is to implement batching in such a way that the processing of the whole dataset is fast, gpu utilization is high and you don't run out of memory.\n","\n","**Hint (general):** group inputs in some specific way to minimize the amount of padding tokens.   \n","**Hint (implementation):** You may (but don't have to) check the implementation of the \"zero-shot-classification\" pipeline in Hugging Face transformers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBFtqBgPNwLG"},"outputs":[],"source":["def zero_shot_classifier_with_batching(\n","    zero_shot_classifier, pipeline_input: Dataset\n",") -> PipelineResult:\n","    \"\"\"A batched ZeroShotClassificationPipeline which processes examples in batches.\n","\n","    Choosing the batch size is part of the function and can be done adaptively.\n","    \"\"\"\n","    #TODO: Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vN7IDLD0NwLG"},"outputs":[],"source":["flush()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Avxo0FujNwLG"},"outputs":[],"source":["%%time\n","r_batched = zero_shot_classifier_with_batching(\n","    zero_shot_classifier, ds.take(256)\n",")"]},{"cell_type":"markdown","metadata":{"id":"Q166omDbNwLG"},"source":["#### Test naive vs batched\n","Write a test checking that naive and vectorized implementations produce same results.\n","\n","**Hint**: there might be some examples in the data which break the comparison.  \n","You may remove them or adjust the function to handle them correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jT1wOCIcNwLG"},"outputs":[],"source":["def compare_naive_and_bathched_zero_shot_classifiers(zero_shot_classifier, data: Dataset):\n","    # TODO: your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoGkyvrqNwLG"},"outputs":[],"source":["compare_naive_and_bathched_zero_shot_classifiers(zero_shot_classifier, ds.shuffle(42).take(256))"]},{"cell_type":"markdown","metadata":{"id":"fFKWwMd4NwLG"},"source":["#### Profiling\n","Profile both implementations with Torch profiler.  \n","Include the results as screenhots and comment on them."]},{"cell_type":"markdown","metadata":{"id":"0vJzkKb1NwLG"},"source":["**TODO:** you profiling results HERE"]},{"cell_type":"markdown","metadata":{"id":"-w2XXhCfNwLG"},"source":["### Process the whole dataset & calculate metrics\n","Here you should process the whole dataset.  \n","Note the time it took.  \n","Then calculate some metrics (accuracy and other you may like) and comment on them.  \n","If you don't have the batched implementation, you may process the dataset with the naive version.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHIlvgCdNwLG"},"outputs":[],"source":["%%time\n","r_batched_whole_dataset = zero_shot_classifier_with_batching(\n","    zero_shot_classifier, ds\n",")"]},{"cell_type":"markdown","metadata":{"id":"_nij5Qf5NwLG"},"source":["**TODO:** your evaluation and comments HERE"]},{"cell_type":"markdown","metadata":{"id":"C2xeH2WqNwLG"},"source":["### Part 3: Decoder Models  \n","\n","In this section, we will explore how to adapt a decoder model to solve this task and how modern LLMs are benchmarked on it.  \n","\n","Recall that decoder models are used for autoregressive text generation, meaning they predict one token at a time, conditioning each prediction on previously generated tokens. A natural way to solve this task would be to prompt the model with different answer options and let it generate a response. However, this approach presents two major challenges:  \n","\n","1. The model may not generate the answer in the expected format, making automatic evaluation difficult.  \n","2. Since decoder models generate text step by step, they do not directly assign a single probability to an entire answer, making it hard to compare different answer choices.  \n","\n","To address this, we use **perplexity** to evaluate how likely the model considers each possible answer.  \n","\n","### Perplexity-Based Evaluation  \n","\n","Since a decoder model predicts a probability distribution over the vocabulary for each token, we can compute the likelihood of any given sequence by multiplying the probabilities assigned to its tokens. Perplexity is a measure of how well the model predicts a sequence, defined as the exponentiated negative average log-likelihood of the sequence. Formally, for a sequence of tokens $\\mathbf{w} = (w_1, w_2, ..., w_n)$, perplexity is computed as:  \n","\n","$\n","PPL(\\mathbf{w}) = \\exp \\left( -\\frac{1}{n} \\sum_{i=1}^{n} \\log P(w_i \\mid w_{<i}) \\right)\n","$\n","\n","where $ P(w_i \\mid w_{<i}) $ is the probability assigned by the model to token $ w_i $ given the preceding tokens.  \n","\n","A lower perplexity score indicates that the model assigns a higher probability to the given answer, making it a more likely choice. By computing perplexity for each possible answer and selecting the one with the lowest value, we can systematically rank the answers without requiring the model to generate them explicitly.  \n","\n","This approach ensures reliable and scalable evaluation, making it a standard technique for benchmarking decoder models on multiple-choice tasks.  \n","\n","You can read more about perplexity and what problems there are when it comes to using it as a metric in [this short blog](https://blog.eleuther.ai/multiple-choice-normalization/). Notice the challenges when it comes to models with different tokenizers and how to overcome them.\n","\n","Last but not least there is reproducibility issue if you deploy big optimized model on moder GPU, you can read more about it [here](https://community.openai.com/t/a-question-on-determinism/8185)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6JWcr--NwLH"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n","model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").eval()"]},{"cell_type":"markdown","metadata":{"id":"R6xSOdN7NwLH"},"source":["#### Revisiting the prompt\n","\n","The prompt and response format also matters. You can read more about that [here](https://huggingface.co/blog/open-llm-leaderboard-mmlu) and also about the differences when it comes to deciding which answer model choosed. You can read in this blog that depending on the prompt and evaluation strategy the benchmark results can vary.\n","\n","We will use HELM prompt and normalize perplexity by token its count."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdY4AK5MNwLH","outputId":"3e63bc7c-07ce-45d4-a589-b5faee040a69"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","The following are multiple choice questions (with answers) about abstract_algebra:\n","\n","Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n","A. 0\n","B. 4\n","C. 2\n","D. 6\n","Answer:\n","\n"]}],"source":["from textwrap import dedent\n","\n","HELM_PROMPT_TEMPLATE = dedent(\"\"\"\n","The following are multiple choice questions (with answers) about {subject}:\n","\n","{question}\n","A. {option_a}\n","B. {option_b}\n","C. {option_c}\n","D. {option_d}\n","Answer:\n","\"\"\")\n","\n","print(\n","    HELM_PROMPT_TEMPLATE.format(\n","        subject=sample_subject,\n","        question=sample_question,\n","        option_a=options[0],\n","        option_b=options[1],\n","        option_c=options[2],\n","        option_d=options[3],\n","    )\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"mv5qgSgjNwLH"},"source":["Let's generate sample answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMUn5OytNwLH","outputId":"40cd1dbe-6914-4790-a46e-25501666270a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Device set to use mps:0\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Attempt 1: \n","Sqrt(1/sqrt) is the number of elements with the same name in a field. Q is a number in the range\n","Attempt 2: \n","E. A for Q Q(x) A E B (X, Y) E f 1 Q (x)\n","\n","E. b\n","Attempt 3: \n","The value of a Q (a) (n) and a B (i) and of a (b) (n) is the value\n","Attempt 4: \n","Let M define an element with four elements containing zero, or M choose the element for which \"M\" is true. (\n","\n","If M\n","Attempt 5: \n","a) Q is less than the mean from the given model, for instance the function Q(x,y) or Q{x,y\n"]}],"source":["generator = pipeline(\"text-generation\", model=\"gpt2\")\n","\n","sample_prompt_formatted = HELM_PROMPT_TEMPLATE.format(\n","    subject=sample_subject,\n","    question=sample_question,\n","    option_a=options[0],\n","    option_b=options[1],\n","    option_c=options[2],\n","    option_d=options[3],\n",")\n","\n","generations = generator(\n","    sample_prompt_formatted, max_new_tokens=30, num_return_sequences=5\n",")\n","\n","for i, generation in enumerate(generations):\n","    print(\n","        f\"Attempt {i+1}:\", generation[\"generated_text\"][len(sample_prompt_formatted) :]\n","    )"]},{"cell_type":"markdown","metadata":{"id":"6N-17qxQNwLH"},"source":["As you can see, if we tried to run it automatically in the background, it would be rather a mess!\n","\n","We start with a simple implementation where we will also utilise [caching](https://huggingface.co/docs/transformers/en/kv_cache) to speed up the process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Rswbem7NwLH","outputId":"e75aa31e-1394-4bad-e3bb-c895affdff2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Scores: [ -9.76306725  -9.76624203  -8.91491032 -10.23246193]\n","Is correct: True\n"]}],"source":["from typing import List\n","\n","from transformers import PreTrainedModel, PreTrainedTokenizer\n","\n","\n","def compute_unnormalised_log_prob_sequentially(\n","    model: PreTrainedModel,\n","    tokenizer: PreTrainedTokenizer,\n","    prompt: str,\n","    completions: List[str],\n","    correct: str,\n","):\n","    \"\"\"\n","    Sequentially computes log probabilities of completions using KV caching.\n","    \"\"\"\n","    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    # Generate KV cache for question - shared part of each completion\n","    with torch.no_grad():\n","        outputs = model(\n","            **prompt_inputs,\n","            use_cache=True,\n","        )\n","        prompt_kv_cache = outputs.past_key_values\n","\n","    log_probs_list = []\n","\n","    # Process all completions sequentially\n","    for completion in completions:\n","        # Tokenize only the completion\n","        completion_inputs = tokenizer(completion, return_tensors=\"pt\").to(model.device)\n","\n","        # Run the model with the cached KV from the prompt\n","        with torch.no_grad():\n","            outputs = model(\n","                input_ids=completion_inputs.input_ids,\n","                past_key_values=prompt_kv_cache,\n","                use_cache=True,\n","            )\n","\n","        logits = outputs.logits\n","\n","        # Compute log probabilities\n","        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n","\n","        # Get log probs of the actual next tokens\n","        token_log_probs = torch.gather(\n","            log_probs,\n","            2,\n","            completion_inputs.input_ids[None, ...],\n","        ).squeeze(-1)\n","\n","        # Sum the log probs to get the sequence log prob\n","        seq_log_prob = token_log_probs.sum()\n","        log_probs_list.append(seq_log_prob.item())\n","\n","    log_probs_list = np.array(log_probs_list)\n","    is_correct = np.argmax(log_probs_list) == ord(\"D\") - ord(correct)\n","    return log_probs_list, is_correct\n","\n","\n","scores_sequential, is_correct = compute_unnormalised_log_prob_sequentially(\n","    model, tokenizer, sample_prompt_formatted, options, answer\n",")\n","\n","print(\"Scores:\", scores_sequential)\n","print(\"Is correct:\", is_correct)"]},{"cell_type":"markdown","metadata":{"id":"Jb2omvRBNwLH"},"source":["##### TASK decoder vectorized:\n","\n","Now your task is to implement vectorized version of this code. We don't want to make forward passes through the model with batch size = 1 in a for loop, that is very inefficient. We want to make forward passes with batch size = number of options (4 in that case).\n","\n","The perplexity calculation after the forward pass doesn't need to be vectorized.\n","\n","    1. Create KV cache with past key values for the shared prompt part - question (2 pkt)\n","    2. Repeat KV cache to make the shapes right for batched options (2 pkt)\n","    3. Calculate perplexity for each option. Make sure not to include padding tokens! (2 pkt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rzsxc-4QNwLI"},"outputs":[],"source":["def compute_unnormalised_log_prob_vectorized(\n","    model: PreTrainedModel,\n","    tokenizer: PreTrainedTokenizer,\n","    prompt: str,\n","    completions: List[str],\n","    correct: str,\n","):\n","    \"\"\"\n","    Computes log probabilities of completions using KV caching with vectorized computation.\n","    \"\"\"\n","    # Tokenize the prompt once\n","    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    # Generate KV cache for question - shared part of each completion\n","    with torch.no_grad():\n","        outputs = model(\n","            **prompt_inputs,\n","            use_cache=True,\n","        )\n","        prompt_kv_cache = outputs.past_key_values\n","\n","    # Tokenize all completions together\n","    # Tokenizer doesn't have padding token, so we use EOS token as padding. It doesn't matter since attention mask will exclude it.\n","    tokenizer.pad_token = tokenizer.eos_token\n","    completion_inputs = tokenizer(\n","        completions, return_tensors=\"pt\", padding=True, truncation=True\n","    ).to(model.device)\n","\n","    # Repeat KV cache for each completion\n","    # Keep in mind that cache is for K and V, for each element of initial batch and for each model layer\n","    batch_size = len(completions)\n","    batched_prompt_kv_cache = \"\"\"\n","        TODO: your code here\n","        \"\"\"\n","\n","    # Run the model with the cached KV from the prompt\n","    with torch.no_grad():\n","        outputs = model(\n","            input_ids=completion_inputs.input_ids,\n","            attention_mask=completion_inputs.attention_mask,\n","            past_key_values=batched_prompt_kv_cache,\n","            use_cache=True,\n","        )\n","\n","    logits = outputs.logits\n","    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n","\n","    # Calculate log probabilities for each completion\n","    log_probs_list = []\n","\n","    for i, completion in enumerate(completions):\n","        \"\"\"\n","        TODO: your code here\n","        \"\"\"\n","\n","        log_probs_list.append(seq_log_prob.item())\n","\n","    log_probs_list = np.array(log_probs_list)\n","    is_correct = np.argmax(log_probs_list) == ord(\"D\") - ord(correct)\n","    return log_probs_list, is_correct\n","\n","\n","scores_vectorized, is_correct = compute_unnormalised_log_prob_vectorized(\n","    model, tokenizer, sample_prompt_formatted, options, answer\n",")\n","\n","print(\"Scores:\", scores_vectorized)\n","print(\"Is correct:\", is_correct)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQZI4HWhNwLN"},"outputs":[],"source":["assert np.allclose(scores_sequential, scores_vectorized)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}