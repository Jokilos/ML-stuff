{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq4OjTS7NwK-"
   },
   "source": [
    "## Project 1\n",
    "# Zero-Shot Question Answering\n",
    "The aim of this project is to get familiar with Language Models for Zero-Shot Question Answering and possible pitfalls when it comes to mesuring LLM performance on common benchmarks.  \n",
    "The project is divided into two parts:\n",
    "1. **Encoder Models**:\n",
    "    - Here you will see how to used predefined HF / transformers classes to solve this task\n",
    "2. **Decoder Models**:\n",
    "    - Here you will see how to adapt a decoder model to solve this task and how are modern LLMs benchmarked on this task.\n",
    "\n",
    "### What is Zero-Shot Question Answering?\n",
    "Zero-shot question answering is a task where a model is given a question and a context, and the model is expected to predict the answer without any training on the context or the question. The model is expected to generalize to unseen context and questions. From practical perspective it is a situation where we want to use a model to our task without any fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIsuYUxQNwLA"
   },
   "source": [
    "### Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qt8nRNCzNwLB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers[torch] in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (2.5.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from transformers[torch]) (1.2.1)\n",
      "Requirement already satisfied: psutil in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[torch]) (6.1.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests->transformers[torch]) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuajMlW3NwLC"
   },
   "source": [
    "### Part 1: Dataset\n",
    "\n",
    "We will work on [MMLU dataset](https://huggingface.co/datasets/CohereForAI/Global-MMLU). Let's have a look at examples from the dataset. For each question we are given 4 answers, the correct one and the subject of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JY0tz1HoNwLC",
    "outputId": "61edfed2-b566-47de-ed89-cbb4fd013afe"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "ds = load_dataset(\"CohereForAI/Global-MMLU\", \"en\", split=\"test\")\n",
    "\n",
    "def preprocess(sample: dict):\n",
    "    return {\n",
    "        \"options\": [\n",
    "            sample[option]\n",
    "            for option in [\"option_a\", \"option_b\", \"option_c\", \"option_d\"]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "ds = ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J6KQ5YgFNwLD",
    "outputId": "23203dc5-597f-4bde-c12f-f953e8af690f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Examples: 14042\n",
      "Mean length: 274.54\n",
      "Max length: 4671\n"
     ]
    }
   ],
   "source": [
    "print(f\"N Examples: {len(ds)}\")\n",
    "print(f\"Mean length: {sum(len(x['question']) for x in ds) / len(ds):4.2f}\")\n",
    "print(f\"Max length: {max(len(x['question']) for x in ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ib3z1nYmNwLD",
    "outputId": "9a67381a-71c3-41cd-b221-009ec4ebab28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "Sample subject: abstract_algebra\n",
      "Options:\n",
      " A: 0\n",
      "B: 4\n",
      "C: 2\n",
      "D: 6\n",
      "Answer: B\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "sample_question = ds[sample_idx][\"question\"]\n",
    "sample_subject = ds[sample_idx][\"subject\"]\n",
    "options = ds[sample_idx][\"options\"]\n",
    "answer = ds[sample_idx][\"answer\"]\n",
    "\n",
    "print(\"Sample question:\", sample_question)\n",
    "print(\"Sample subject:\", sample_subject)\n",
    "print(\"Options:\\n\", \"\\n\".join([f\"{c.upper()}: {o}\" for c, o in zip(\"abcd\", options)]))\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2E1t7b9NwLE"
   },
   "source": [
    "### Part 2: Encoder Models\n",
    "\n",
    "Let's have a look how to use out of the box transformers pipeline to solve this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2bYz1dJ9NwLE",
    "outputId": "124a266a-ee51-4690-9ed4-7446228344aa"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `modernbert` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Git/ML-stuff/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1038\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;129m@replace_list_option_in_docstrings\u001b[39m()\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    998\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;124;03m    Instantiate one of the configuration classes of the library from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \n\u001b[1;32m   1001\u001b[0m \u001b[38;5;124;03m    The configuration class to instantiate is selected based on the `model_type` property of the config object that\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;124;03m    is loaded, or when it's missing, by falling back to using pattern matching on `pretrained_model_name_or_path`:\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m    List options\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m        pretrained_model_name_or_path (`str` or `os.PathLike`):\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m            Can be either:\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \n\u001b[1;32m   1010\u001b[0m \u001b[38;5;124;03m                - A string, the *model id* of a pretrained model configuration hosted inside a model repo on\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;124;03m                  huggingface.co.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;124;03m                - A path to a *directory* containing a configuration file saved using the\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;124;03m                  [`~PretrainedConfig.save_pretrained`] method, or the [`~PreTrainedModel.save_pretrained`] method,\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;124;03m                  e.g., `./my_model_directory/`.\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03m                - A path or url to a saved configuration JSON *file*, e.g.,\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m                  `./my_model_directory/configuration.json`.\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m        cache_dir (`str` or `os.PathLike`, *optional*):\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m            Path to a directory in which a downloaded pretrained model configuration should be cached if the\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;124;03m            standard cache should not be used.\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;124;03m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03m            Whether or not to force the (re-)download the model weights and configuration files and override the\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;124;03m            cached versions if they exist.\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;124;03m        resume_download:\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m            Deprecated and ignored. All downloads are now resumed by default when possible.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m            Will be removed in v5 of Transformers.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03m        proxies (`Dict[str, str]`, *optional*):\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m        revision (`str`, *optional*, defaults to `\"main\"`):\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;124;03m            identifier allowed by git.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;124;03m        return_unused_kwargs (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m            If `False`, then this function returns just the final configuration object.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m            If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;124;03m            dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;124;03m            part of `kwargs` which has not been used to update `config` and is otherwise ignored.\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m        trust_remote_code (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03m            Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m            should only be set to `True` for repositories you trust and in which you have read the code, as it will\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m            execute code present on the Hub on your local machine.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m        kwargs(additional keyword arguments, *optional*):\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;124;03m            The values in kwargs of any keys which are configuration attributes will be used to override the loaded\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;124;03m            values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;124;03m            by the `return_unused_kwargs` keyword parameter.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \n\u001b[1;32m   1050\u001b[0m \u001b[38;5;124;03m    ```python\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m    >>> from transformers import AutoConfig\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03m    >>> # Download configuration from huggingface.co and cache.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"google-bert/bert-base-uncased\")\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    >>> # Download configuration from huggingface.co (user-uploaded) and cache.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"dbmdz/bert-base-german-cased\")\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m    >>> # If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"./test/bert_saved_model/\")\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m    >>> # Load a specific configuration file.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"./test/bert_saved_model/my_configuration.json\")\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03m    >>> # Change some config attributes when loading a pretrained config.\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;124;03m    >>> config = AutoConfig.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True, foo=False)\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03m    >>> config.output_attentions\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    >>> config, unused_kwargs = AutoConfig.from_pretrained(\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    ...     \"google-bert/bert-base-uncased\", output_attentions=True, foo=False, return_unused_kwargs=True\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m    >>> config.output_attentions\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \n\u001b[1;32m   1076\u001b[0m \u001b[38;5;124;03m    >>> unused_kwargs\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;124;03m    {'foo': False}\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m     use_auth_token \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_auth_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Git/ML-stuff/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:740\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    741\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'modernbert'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline, set_seed\n\u001b[1;32m      3\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m zero_shot_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero-shot-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMoritzLaurer/ModernBERT-large-zeroshot-v2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git/ML-stuff/venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:849\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    847\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 849\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    854\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Git/ML-stuff/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1040\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m-> 1040\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1041\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1042\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1043\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1044\u001b[0m         )\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `modernbert` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\", model=\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THHido-rNwLE",
    "outputId": "bd3b3e6b-8191-481e-dce9-fb81b07f3907"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.',\n",
       " 'labels': ['6', '4', '2', '0'],\n",
       " 'scores': [0.27904871106147766,\n",
       "  0.275924950838089,\n",
       "  0.23752973973751068,\n",
       "  0.20749661326408386]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_classifier(\n",
    "    sample_question,\n",
    "    options,\n",
    "    hypothesis_template=\"The correct answer is: {}\",\n",
    "    multi_label=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDESO1FNwLF"
   },
   "source": [
    "#### How it works under the hood?\n",
    "\n",
    "If you go to the [source code](https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src/transformers/pipelines/zero_shot_classification.py#L49) you can see that it uses `ModelForSequenceClassification` and in [model card](https://huggingface.co/MoritzLaurer/ModernBERT-large-zeroshot-v2.0) you can read that the model was in fact fine tuned on question answering task.  \n",
    "The base model used for fine-tuning is ModernBERT, which is a modernized version of the BERT model, making use of various advancements in the *atention* mechanism, improving both performance and efficiency.  \n",
    "If you are interested in details, we highly recommend the following [Hugging Face blogpost](https://huggingface.co/blog/modernbert).\n",
    "\n",
    "By digging deeper in [model config](https://huggingface.co/MoritzLaurer/ModernBERT-large-zeroshot-v2.0/blob/a51e07b524299e309dd2b88d48b0cfa2bd9ec598/config.json#L24) we can see that the only labels the model knows about are\n",
    "```\n",
    "\"id2label\": {\n",
    "    \"0\": \"entailment\",\n",
    "    \"1\": \"not_entailment\"\n",
    "  }\n",
    "```\n",
    "\n",
    "For each option the model classifies the text\n",
    "```\n",
    "{question}\n",
    "{hypothesis_template} {option}\n",
    "```\n",
    "as either entailment or not entailment. The option with the highest entailment score is the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbxpBtZSNwLF"
   },
   "source": [
    "#### Task: evaluate the model on the dataset\n",
    "Your task is to evaluate the model on the dataset and calculate some metrics (accuracy, potentially some other metrics and more granular insight - e.g. per question subject).  \n",
    "Additionally you will implement batching to improve the evaluation performance and use profiler to analyze the improvements.\n",
    "\n",
    "Note that our problem is not typical classification task because the classes (here: available answers) are different for each question.  \n",
    "The \"zero-shot-classification\" pipeline expects that the *classes* passed to it are the same for all examples in the batch.  \n",
    "To overcome this limitation we need to reimplement the pipeline.\n",
    "\n",
    "The task involves the following steps:\n",
    "\n",
    "    1. First, implement a naive function which given the dataset (or its subset) processes it row by row using the zero-shot pipeline. (1 pkt)\n",
    "    2. Implement a vectorized (batched) version of the pipeline. (4 pkt)\n",
    "    3. Write a test function comparing the results of batching with the naive version. (1 pkt)\n",
    "    4. Profile the batched version and (adaptively) choose the best batch size for processing the whole dataset. (2 pkt)\n",
    "    5. Calculate accuracy of the model and some more insight on the results. (2pkt)\n",
    "        Batching is not strictly required for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upTd20U_NwLF"
   },
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzNglZxvNwLF"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from textwrap import dedent\n",
    "import torch\n",
    "\n",
    "\n",
    "QUESTION_TEMPLATE = dedent(\n",
    "    \"\"\"\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "HYPOTHESIS_TEMPLATE = \"The correct answer is: {}\"\n",
    "\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IukVvAFdNwLF"
   },
   "source": [
    "#### Naive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipOF0loWNwLF"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class PipelineResult(TypedDict):\n",
    "    labels: list[list[str]] #sorted according to scores\n",
    "    scores: list[list[float]] #sorted descending\n",
    "    top_inds: list[list[int]] #for each label, its index in the input's options list\n",
    "\n",
    "\n",
    "def naive_zero_shot_classifier_pipeline(zero_shot_classifier, pipeline_input: Dataset) -> PipelineResult:\n",
    "    \"\"\"A naive ZeroShotClassificationPipeline which iterates over examples and processes them one by one.\"\"\"\n",
    "\n",
    "    # TODO: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XSC9oBqNwLF"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "r = naive_zero_shot_classifier_pipeline(\n",
    "    zero_shot_classifier, ds.take(256)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfG0cp3RNwLG"
   },
   "source": [
    "#### Batched implementation\n",
    "Rewrite the pipeline to process the dataset in batches to improve efficiency.  \n",
    "ModernBERT supports a special batching mode called *sequence packing* but its usage requires FlashAttention and is beyond the scope of this task.  \n",
    "Your goal is to implement batching in such a way that the processing of the whole dataset is fast, gpu utilization is high and you don't run out of memory.\n",
    "\n",
    "**Hint (general):** group inputs in some specific way to minimize the amount of padding tokens.   \n",
    "**Hint (implementation):** You may (but don't have to) check the implementation of the \"zero-shot-classification\" pipeline in Hugging Face transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBFtqBgPNwLG"
   },
   "outputs": [],
   "source": [
    "def zero_shot_classifier_with_batching(\n",
    "    zero_shot_classifier, pipeline_input: Dataset\n",
    ") -> PipelineResult:\n",
    "    \"\"\"A batched ZeroShotClassificationPipeline which processes examples in batches.\n",
    "\n",
    "    Choosing the batch size is part of the function and can be done adaptively.\n",
    "    \"\"\"\n",
    "    #TODO: Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vN7IDLD0NwLG"
   },
   "outputs": [],
   "source": [
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Avxo0FujNwLG"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "r_batched = zero_shot_classifier_with_batching(\n",
    "    zero_shot_classifier, ds.take(256)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q166omDbNwLG"
   },
   "source": [
    "#### Test naive vs batched\n",
    "Write a test checking that naive and vectorized implementations produce same results.\n",
    "\n",
    "**Hint**: there might be some examples in the data which break the comparison.  \n",
    "You may remove them or adjust the function to handle them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jT1wOCIcNwLG"
   },
   "outputs": [],
   "source": [
    "def compare_naive_and_bathched_zero_shot_classifiers(zero_shot_classifier, data: Dataset):\n",
    "    # TODO: your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoGkyvrqNwLG"
   },
   "outputs": [],
   "source": [
    "compare_naive_and_bathched_zero_shot_classifiers(zero_shot_classifier, ds.shuffle(42).take(256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFKWwMd4NwLG"
   },
   "source": [
    "#### Profiling\n",
    "Profile both implementations with Torch profiler.  \n",
    "Include the results as screenhots and comment on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vJzkKb1NwLG"
   },
   "source": [
    "**TODO:** you profiling results HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-w2XXhCfNwLG"
   },
   "source": [
    "### Process the whole dataset & calculate metrics\n",
    "Here you should process the whole dataset.  \n",
    "Note the time it took.  \n",
    "Then calculate some metrics (accuracy and other you may like) and comment on them.  \n",
    "If you don't have the batched implementation, you may process the dataset with the naive version.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHIlvgCdNwLG"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "r_batched_whole_dataset = zero_shot_classifier_with_batching(\n",
    "    zero_shot_classifier, ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nij5Qf5NwLG"
   },
   "source": [
    "**TODO:** your evaluation and comments HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2xeH2WqNwLG"
   },
   "source": [
    "### Part 3: Decoder Models  \n",
    "\n",
    "In this section, we will explore how to adapt a decoder model to solve this task and how modern LLMs are benchmarked on it.  \n",
    "\n",
    "Recall that decoder models are used for autoregressive text generation, meaning they predict one token at a time, conditioning each prediction on previously generated tokens. A natural way to solve this task would be to prompt the model with different answer options and let it generate a response. However, this approach presents two major challenges:  \n",
    "\n",
    "1. The model may not generate the answer in the expected format, making automatic evaluation difficult.  \n",
    "2. Since decoder models generate text step by step, they do not directly assign a single probability to an entire answer, making it hard to compare different answer choices.  \n",
    "\n",
    "To address this, we use **perplexity** to evaluate how likely the model considers each possible answer.  \n",
    "\n",
    "### Perplexity-Based Evaluation  \n",
    "\n",
    "Since a decoder model predicts a probability distribution over the vocabulary for each token, we can compute the likelihood of any given sequence by multiplying the probabilities assigned to its tokens. Perplexity is a measure of how well the model predicts a sequence, defined as the exponentiated negative average log-likelihood of the sequence. Formally, for a sequence of tokens $\\mathbf{w} = (w_1, w_2, ..., w_n)$, perplexity is computed as:  \n",
    "\n",
    "$\n",
    "PPL(\\mathbf{w}) = \\exp \\left( -\\frac{1}{n} \\sum_{i=1}^{n} \\log P(w_i \\mid w_{<i}) \\right)\n",
    "$\n",
    "\n",
    "where $ P(w_i \\mid w_{<i}) $ is the probability assigned by the model to token $ w_i $ given the preceding tokens.  \n",
    "\n",
    "A lower perplexity score indicates that the model assigns a higher probability to the given answer, making it a more likely choice. By computing perplexity for each possible answer and selecting the one with the lowest value, we can systematically rank the answers without requiring the model to generate them explicitly.  \n",
    "\n",
    "This approach ensures reliable and scalable evaluation, making it a standard technique for benchmarking decoder models on multiple-choice tasks.  \n",
    "\n",
    "You can read more about perplexity and what problems there are when it comes to using it as a metric in [this short blog](https://blog.eleuther.ai/multiple-choice-normalization/). Notice the challenges when it comes to models with different tokenizers and how to overcome them.\n",
    "\n",
    "Last but not least there is reproducibility issue if you deploy big optimized model on moder GPU, you can read more about it [here](https://community.openai.com/t/a-question-on-determinism/8185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6JWcr--NwLH"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6xSOdN7NwLH"
   },
   "source": [
    "#### Revisiting the prompt\n",
    "\n",
    "The prompt and response format also matters. You can read more about that [here](https://huggingface.co/blog/open-llm-leaderboard-mmlu) and also about the differences when it comes to deciding which answer model choosed. You can read in this blog that depending on the prompt and evaluation strategy the benchmark results can vary.\n",
    "\n",
    "We will use HELM prompt and normalize perplexity by token its count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdY4AK5MNwLH",
    "outputId": "3e63bc7c-07ce-45d4-a589-b5faee040a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following are multiple choice questions (with answers) about abstract_algebra:\n",
      "\n",
      "Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "A. 0\n",
      "B. 4\n",
      "C. 2\n",
      "D. 6\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "HELM_PROMPT_TEMPLATE = dedent(\"\"\"\n",
    "The following are multiple choice questions (with answers) about {subject}:\n",
    "\n",
    "{question}\n",
    "A. {option_a}\n",
    "B. {option_b}\n",
    "C. {option_c}\n",
    "D. {option_d}\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "print(\n",
    "    HELM_PROMPT_TEMPLATE.format(\n",
    "        subject=sample_subject,\n",
    "        question=sample_question,\n",
    "        option_a=options[0],\n",
    "        option_b=options[1],\n",
    "        option_c=options[2],\n",
    "        option_d=options[3],\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv5qgSgjNwLH"
   },
   "source": [
    "Let's generate sample answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMUn5OytNwLH",
    "outputId": "40cd1dbe-6914-4790-a46e-25501666270a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: \n",
      "Sqrt(1/sqrt) is the number of elements with the same name in a field. Q is a number in the range\n",
      "Attempt 2: \n",
      "E. A for Q Q(x) A E B (X, Y) E f 1 Q (x)\n",
      "\n",
      "E. b\n",
      "Attempt 3: \n",
      "The value of a Q (a) (n) and a B (i) and of a (b) (n) is the value\n",
      "Attempt 4: \n",
      "Let M define an element with four elements containing zero, or M choose the element for which \"M\" is true. (\n",
      "\n",
      "If M\n",
      "Attempt 5: \n",
      "a) Q is less than the mean from the given model, for instance the function Q(x,y) or Q{x,y\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "sample_prompt_formatted = HELM_PROMPT_TEMPLATE.format(\n",
    "    subject=sample_subject,\n",
    "    question=sample_question,\n",
    "    option_a=options[0],\n",
    "    option_b=options[1],\n",
    "    option_c=options[2],\n",
    "    option_d=options[3],\n",
    ")\n",
    "\n",
    "generations = generator(\n",
    "    sample_prompt_formatted, max_new_tokens=30, num_return_sequences=5\n",
    ")\n",
    "\n",
    "for i, generation in enumerate(generations):\n",
    "    print(\n",
    "        f\"Attempt {i+1}:\", generation[\"generated_text\"][len(sample_prompt_formatted) :]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N-17qxQNwLH"
   },
   "source": [
    "As you can see, if we tried to run it automatically in the background, it would be rather a mess!\n",
    "\n",
    "We start with a simple implementation where we will also utilise [caching](https://huggingface.co/docs/transformers/en/kv_cache) to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Rswbem7NwLH",
    "outputId": "e75aa31e-1394-4bad-e3bb-c895affdff2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ -9.76306725  -9.76624203  -8.91491032 -10.23246193]\n",
      "Is correct: True\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def compute_unnormalised_log_prob_sequentially(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    prompt: str,\n",
    "    completions: List[str],\n",
    "    correct: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sequentially computes log probabilities of completions using KV caching.\n",
    "    \"\"\"\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate KV cache for question - shared part of each completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **prompt_inputs,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        prompt_kv_cache = outputs.past_key_values\n",
    "\n",
    "    log_probs_list = []\n",
    "\n",
    "    # Process all completions sequentially\n",
    "    for completion in completions:\n",
    "        # Tokenize only the completion\n",
    "        completion_inputs = tokenizer(completion, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Run the model with the cached KV from the prompt\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=completion_inputs.input_ids,\n",
    "                past_key_values=prompt_kv_cache,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute log probabilities\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Get log probs of the actual next tokens\n",
    "        token_log_probs = torch.gather(\n",
    "            log_probs,\n",
    "            2,\n",
    "            completion_inputs.input_ids[None, ...],\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Sum the log probs to get the sequence log prob\n",
    "        seq_log_prob = token_log_probs.sum()\n",
    "        log_probs_list.append(seq_log_prob.item())\n",
    "\n",
    "    log_probs_list = np.array(log_probs_list)\n",
    "    is_correct = np.argmax(log_probs_list) == ord(\"D\") - ord(correct)\n",
    "    return log_probs_list, is_correct\n",
    "\n",
    "\n",
    "scores_sequential, is_correct = compute_unnormalised_log_prob_sequentially(\n",
    "    model, tokenizer, sample_prompt_formatted, options, answer\n",
    ")\n",
    "\n",
    "print(\"Scores:\", scores_sequential)\n",
    "print(\"Is correct:\", is_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb2omvRBNwLH"
   },
   "source": [
    "##### TASK decoder vectorized:\n",
    "\n",
    "Now your task is to implement vectorized version of this code. We don't want to make forward passes through the model with batch size = 1 in a for loop, that is very inefficient. We want to make forward passes with batch size = number of options (4 in that case).\n",
    "\n",
    "The perplexity calculation after the forward pass doesn't need to be vectorized.\n",
    "\n",
    "    1. Create KV cache with past key values for the shared prompt part - question (2 pkt)\n",
    "    2. Repeat KV cache to make the shapes right for batched options (2 pkt)\n",
    "    3. Calculate perplexity for each option. Make sure not to include padding tokens! (2 pkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rzsxc-4QNwLI"
   },
   "outputs": [],
   "source": [
    "def compute_unnormalised_log_prob_vectorized(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    prompt: str,\n",
    "    completions: List[str],\n",
    "    correct: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes log probabilities of completions using KV caching with vectorized computation.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt once\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate KV cache for question - shared part of each completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **prompt_inputs,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        prompt_kv_cache = outputs.past_key_values\n",
    "\n",
    "    # Tokenize all completions together\n",
    "    # Tokenizer doesn't have padding token, so we use EOS token as padding. It doesn't matter since attention mask will exclude it.\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    completion_inputs = tokenizer(\n",
    "        completions, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Repeat KV cache for each completion\n",
    "    # Keep in mind that cache is for K and V, for each element of initial batch and for each model layer\n",
    "    batch_size = len(completions)\n",
    "    batched_prompt_kv_cache = \"\"\"\n",
    "        TODO: your code here\n",
    "        \"\"\"\n",
    "\n",
    "    # Run the model with the cached KV from the prompt\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=completion_inputs.input_ids,\n",
    "            attention_mask=completion_inputs.attention_mask,\n",
    "            past_key_values=batched_prompt_kv_cache,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    logits = outputs.logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Calculate log probabilities for each completion\n",
    "    log_probs_list = []\n",
    "\n",
    "    for i, completion in enumerate(completions):\n",
    "        \"\"\"\n",
    "        TODO: your code here\n",
    "        \"\"\"\n",
    "\n",
    "        log_probs_list.append(seq_log_prob.item())\n",
    "\n",
    "    log_probs_list = np.array(log_probs_list)\n",
    "    is_correct = np.argmax(log_probs_list) == ord(\"D\") - ord(correct)\n",
    "    return log_probs_list, is_correct\n",
    "\n",
    "\n",
    "scores_vectorized, is_correct = compute_unnormalised_log_prob_vectorized(\n",
    "    model, tokenizer, sample_prompt_formatted, options, answer\n",
    ")\n",
    "\n",
    "print(\"Scores:\", scores_vectorized)\n",
    "print(\"Is correct:\", is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQZI4HWhNwLN"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(scores_sequential, scores_vectorized)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
