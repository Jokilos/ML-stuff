<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="F. Plata, K. Iwanicki, M. Banaszek, W. Ciszewski" />
  <title>Distributed Systems Lab 13</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <style type="text/css">
body
{
font-family: Helvetica, sans;
background-color: #f0f0f0;
font-size: 12pt;
color: black;
text-decoration: none;
font-weight: normal;
}
section.content {
width: 19cm;
font-size: 12pt;
text-align: justify;
margin-left: auto;
margin-right: auto;
margin-top: 20pt;
background-color: white;
padding: 20pt;
}
h1
{
font-size: xx-large;
text-decoration: none;
font-weight: bold;
text-align: center;
}
h2
{
font-size: xx-large;
text-decoration: none;
font-weight: bold;
text-align: left;
border-bottom: 1px solid #808080;
}
h3
{
font-size: x-large;
text-decoration: none;
font-weight: bold;
text-align: left;
}
h1 + h3 {
text-align: center;
}
h4
{
font-size: large;
text-decoration: none;
font-weight: bold;
text-align: left;
}
h5
{
font-size: medium;
text-decoration: none;
font-weight: bold;
text-align: left;
}
h6
{
font-size: medium;
text-decoration: none;
font-weight: normal;
text-align: left;
}
table
{
border-width: 1px;
border-spacing: 0px;
border-style: solid;
border-color: #808080;
border-collapse: collapse;
font-family: Times, serif;
font-size: 12pt;
color: black;
text-decoration: none;
font-weight: normal;
background-color: white;
}
td
{
border-width: 1px;
border-style: solid;
border-color: #808080;
padding: 3pt;
background-color: white;
}
th
{
border-width: 1px;
border-style: solid;
border-color: #808080;
padding: 3pt;
font-weight: bold;
background-color: #f0f0f0;
}
a:link {
color: blue;
text-decoration: none;
font-weight: normal;
}
a:visited {
color: blue;
text-decoration: none;
font-weight: normal;
}
a:hover {
text-decoration: underline;
font-weight: normal;
}
pre.sourceCode {
font-size: 90%;
}
</style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="distributed-systems-lab-13" class="content">
<h1>Distributed Systems Lab 13</h1>
<h3 id="eventually-consistent-gossip-based-aggregation">Eventually consistent gossip-based aggregation</h3>
<p>During the previous labs, we discussed eventual consistency and, more specifically, its op-based variant. In that variant, by means of reliable broadcast, processes constituting the system exchange client operations that are subsequently applied to their local states, so that in effect those states converge to the same values. This lab, in turn, focuses on state-based eventual consistency, in which process states themselves are being exchanged. As an application example, we will adopt gossip-based aggregation. We will use files contained in <a href="./dslab13.tgz">this package</a>, which you should download and extract locally.</p>
<h2 id="learning-section">Learning Section</h2>
<h3 id="in-network-aggregation">In-network aggregation</h3>
<p><strong>Aggregation</strong> describes functionality of summarizing information. There are many functions that can be utilized for producing various types of summaries, but during this lab we will be interested in basic ones, namely <code>MIN</code>, <code>MAX</code>, <code>COUNT</code>, <code>SUM</code>, and <code>AVG</code>. Without loss of generality, we assume that each process <code>i</code> holds a single value, <code>vi</code>, and the goal is to compute the value of such a function over the set comprising the values of all processes, <code>V = {v1, v2, ..., vN}</code>. In our distributed setting, a process will correspond to a network node, and the nodes communicate only by exchanging messages.</p>
<p>A straightforward approach to aggregating set <code>V</code> can be as follows: all network nodes send their raw values to a distinguished node, and the distinguished node computes the desired aggregation function on the entire value set, possibly disseminating the result back to the individual nodes if necessary. Such a centralized approach, however, may not be appropriate in some scenarios, notably when the values, <code>vi</code>, are large or somehow bound to their nodes. In such cases, distributed <strong>in-network aggregation</strong> may be required.</p>
<p>Distributed in-network computation of the aforementioned aggregation functions relies on so-called <em>partial aggregates</em>. Partial aggregates summarize information from subsets of nodes, can be combined to represent summaries of larger subsets, and final aggregate values are computed from them. More formally, the following operations are defined on partial aggregates:</p>
<ul>
<li><code>init(v) -&gt; P</code> – given the value, <code>v</code>, of the executing node, initializes a partial aggregate, <code>P</code>, which will represent a singleton subset containing the value of the node.</li>
<li><code>merge(P, P) -&gt; P</code> – merges two partial aggregates for two subsets of node values into a partial aggregate for the subset being the union of the two subsets.</li>
<li><code>eval(P) -&gt; a</code> – returns the final result, <code>a</code>, of the computed aggregation function for the subset of node values corresponding to the partial aggregate, <code>P</code>.</li>
</ul>
<p>For example, when computing <code>MAX</code>, the partial aggregate would be simply an element of set <code>V</code>, and the three operations would be defined as follows: <code>init(v) = v</code>, <code>merge(vx, vy) = max(vx, vy)</code>, <code>eval(v) = v</code>.</p>
<h3 id="decentralized-gossip-based-aggregation">Decentralized gossip-based aggregation</h3>
<p>A common approach to performing in-network aggregation entails organizing the nodes into a logical spanning tree. Each node in the tree <code>init</code>ializes its own partial aggregate based on its local value. Then, the leaf nodes send their partial aggregates to their parent nodes in the tree. Such a node <code>merge</code>s the partial aggregates received from its children into its local partial aggregate, which it then forwards to its parent node in the tree, and so on, up to the root node of the tree. Finally, the root node <code>eval</code>uates the final aggregation result from its local partial aggregate and, if necessary, disseminates this value to the individual nodes.</p>
<p>Such an approach is well suited when the node population and connectivity are fairly stable. In contrast, if they are dynamic, maintaining the tree can become a major problem. In such scenarios, more flexible communication algorithms are employed, which in essence offer multiple paths for information to flow from one node to another. A class particularly popular in large-scale highly dynamic systems is <strong>epidemic algorithms</strong>, also known as <strong>gossip-based algorithms</strong> or <strong>gossiping</strong> for short.</p>
<p>In such an algorithm, nodes operate in rounds, each round lasting some fixed time interval. In every round, each node selects at random another node in the network. This is frequently a task of a so-called <a href="https://dl.acm.org/doi/abs/10.1145/1275517.1275520">peer sampling service</a>, which, interestingly, need not be aware of the entire node population. The node then either sends its local information to that node (<em>push-based gossiping</em>), fetches information from that node (<em>pull-based gossiping</em>), or exchanges information with that node (<em>push-pull-based gossiping</em>). It has been shown that such algorithms propagate information in the network exponentially fast, like epidemics or gossip, and hence their name. They have numerous applications, in-network aggregation being one of them.</p>
<p>In particular, a distributed computation of an aggregation function using gossiping can proceed roughly as follows. Each node <code>init</code>ializes its own local partial aggregate based on its local value. The nodes then repeatedly gossip their local partial aggregates, such that upon reception of a partial aggregate from another node in a given round, the recipient <code>merge</code>s it into the local one, to be used in the next round, and so on. The gossiping is in principle an infinite process but should eventually converge, and thus, if sufficiently many rounds have passed, any node (not just some specific one) can <code>eval</code>uate its local partial aggregate into the final aggregation result. In practice, the evaluation can happen in each round to give an additional indication of whether the results have converged sufficiently.</p>
<h3 id="probabilistic-counting-sketches">Probabilistic counting sketches</h3>
<p>Such a gossip-based in-network aggregation algorithm can be directly applied to <code>MIN</code> and <code>MAX</code>. In contrast, employing it for the other considered aggregation functions is more involved. This is due to the multi-path propagation of information during gossiping, which may cause the value of a given node to contribute multiple times to the final aggregate, thereby distorting the counts, sums, and averages. This problem can be addressed by having partial aggregates, and notably, their <code>merge</code> operation, <em>order- and duplicate-insensitive</em>. In other words, partial aggregates can be made monotonic and the <code>merge</code> operation can be made:</p>
<ul>
<li>idempotent – <code>merge(P1, P1) = P1</code>,</li>
<li>commutative – <code>merge(P1, P2) = merge(P2, P1)</code>, and</li>
<li>associative – <code>merge(P1, merge(P2, P3)) = merge(merge(P1, P2), P3)</code>.</li>
</ul>
<p>A straightforward way of ensuring these properties could be by having partial aggregates correspond to their respective value subsets and the <code>merge</code> operation to a set union. However, in effect, the state exchanged between the nodes during gossiping would be linear in the number of nodes, thereby impairing scalability. Therefore, we want partial aggregates to somehow summarize information as well. To explain how this can be done, let us focus on the <code>COUNT</code> function, as other functions can be implemented using this function.</p>
<p>Order- and duplicate-insensitive counting can be realized by so-called <strong>probabilistic counting sketches/synopses</strong>. They do not produce exact values but only estimates. Nevertheless, in a dynamic system, estimates are typically sufficient, as exact counts could be difficult to obtain anyway. There are many algorithms for computing such approximate counts, notably <a href="https://dl.acm.org/doi/abs/10.1145/78922.78925">linear counting</a> and <a href="https://dmtcs.episciences.org/3545/pdf">hyper-log-log</a> are widely recognized ones. Here, we focus on a variant of <a href="https://www.sciencedirect.com/science/article/pii/0022000085900418">probabilistic counting</a>.</p>
<p>In this approach, a partial aggregate comprises one or more <em>instances</em> of a same-sized <em>sketch</em>. A sketch is a bitmask logarithmic in the number of counted values. The three operations on such bitmasks are defined as follows:</p>
<ul>
<li><code>init(_) -&gt; P</code> – Produces a bitmask that has <code>0</code> on all positions but one. The position of the sole <code>1</code> is selected at random from a geometric distribution with parameter <code>1/2</code>, that is, position 0 is selected with probability <code>1/2</code>, position 1, with probability <code>1/4</code>, position 2, with probability <code>1/8</code>, and so on.</li>
<li><code>merge(P, P) -&gt; P</code> – Returns a bitmask that is a bit-wise OR of the two bitmasks given as parameters.</li>
<li><code>eval(P) -&gt; a</code> – Yields value <code>a</code> equal to <code>c * 2^Fz</code>, where <code>Fz</code> is the position in the bitmask of the first <code>0</code> bit and <code>c = 1.29281</code> is a scaling factor. Value <code>a</code> represents the result of counting.</li>
</ul>
<p>All in all, rather than an exact count, a sketch aims to estimate its order of magnitude (base 2), that is, its base-2 logarithm. This may result in large errors. They can be alleviated by using multiple sketch instances per partial aggregate. The three operations extend to such a vector of sketch instances as follows:</p>
<ul>
<li><code>init</code> uses the previous algorithm independently for each instance in the vector, thereby setting one bit to <code>1</code> in each instance.</li>
<li><code>merge</code> also performs the bitwise OR independently for each element of the vectors of instances constituting the partial aggregates: instance 0 in the first aggregate is ORed with instance 0 in the second aggregate, instance 1 with instance 1, and so on.</li>
<li><code>eval</code> yields a geometric average of the values computed using the previous algorithm, that is, a value equal to <code>c * 2^((Fz0 + Fz1 + ... + FzM)/(M+1))</code>, where <code>M+1</code> is the number of sketch instances in a partial aggregate, and <code>Fzi</code> is the index of the first <code>0</code> bit in instance <code>i</code>.</li>
</ul>
<h2 id="small-assignment">Small Assignment</h2>
<p>Your task is to implement gossip-based aggregation using probabilistic counting sketches as discussed hitherto. The implementation must be based on the supplied template, which assumes a system consisting of <code>Node</code>s and clients that operate as follows.</p>
<p>A client can contact any node and request it to install an aggregation query, which is done by sending to the node a <code>QueryInstallMsg</code>. The goal of the query is estimating the number of nodes in the system that satisfy the associated <code>predicate</code>. The query also specifies the number of probabilistic sketch instances (<code>num_instances</code>) and bits in each instance (<code>bits_per_instance</code>) that should be utilized when computing the aggregate.</p>
<p>Nodes use gossiping to propagate among each other the information necessary for executing such queries. A node initiates gossiping whenever it receives a <code>SyncTriggerMsg</code>. It is the system that controls when those messages are sent; you must never send them in your implementation. Upon reception of such a message, the node requests its associated peer sampling service (<code>pss</code>) to obtain a random node with which it will gossip. Subsequently, it sends the necessary information to that node within a single <code>SyncGossipMsg</code>. The other node does not reply with any message, that is, the communication scheme is push-based.</p>
<p>At any moment in time, a client can contact any node asking it to provide the current estimate of the result for any query. The client does this by sending to the node a <code>QueryResultPollMsg</code>. The message carries the identifier of the node on which the query was originally installed (<code>initiator</code>) and a callback to be executed by the receiving node with the query result estimate as the parameter (<code>callback</code>). If the node is not aware of the query, it executes the callback with value <code>None</code>; otherwise, it executes the callback with its current estimate of the node count satisfying the predicate of the query.</p>
<p>To recap, multiple queries can be executing in the system at the same time. However, a next query installed by a client at a node overrides the previous query installed at that node by the same or another client: the execution of the previous query should eventually cease. In other words, for every initiator, each node executes only the most recent query of which it is aware. Finally, a <code>SyncGossipMsg</code>, sent by a node to its peer, should contain information on all queries executed by the node.</p>
<p>Your solution must implement probabilistic counting by means of the <code>ProbabilisticCounter</code> structure. As the source of random values (<code>RandomnessSource</code>) for selecting sketch bits, the dedicated <code>rs</code> object of the node must be utilized. The object generates pseudo-random values from a uniform distribution. To convert them into ones from the desired geometric distribution, your solution must use the provided <code>uniform_u32_to_geometric</code> method. Moreover, <code>ProbabilisticCounter</code> must ensure special handling of two corner cases. First, <code>evaluate</code> on a counter in which no sketch instance has any bit set to <code>1</code> should return <code>0</code> instead of the value implied by the previous formula. Second, <code>evaluate</code> on a counter in which at least one instance has all bits set to <code>1</code> should return infinity (<code>u64::MAX</code>) instead of the value from the formula.</p>
<p>This Small Assignment is worth <strong><span style="color:red">2 points</span></strong>. To run the system you should use the executor system you implemented as the first Large Assignment.</p>
<h2 id="additional-homework">Additional Homework</h2>
<p>When you have solved the assignment, think how to use partial aggregates for <code>COUNT</code> to implement <code>SUM</code> and <code>AVG</code>.</p>
<p>Finally, gossiping and aggregation are large and important areas in distributed systems. If you are interested, explore the cited papers as well as others concerned with the topics.</p>
<hr />
<p>Authors: K. Iwanicki, M. Banaszek, W. Ciszewski.</p>
</section>
</body>
</html>
